{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20994,
     "status": "ok",
     "timestamp": 1749570071804,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "SZnSaEPZfLiW",
    "outputId": "3f822441-0b51-4fee-a9d2-c33e7a1182ca"
   },
   "outputs": [],
   "source": [
    "# !pip install pykrige\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.uk import UniversalKriging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Reshape, Dropout, LSTM,\n",
    "    RepeatVector\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate, Input, ZeroPadding2D, Cropping2D\n",
    "from tensorflow.keras.models import Model\n",
    "import itertools\n",
    "from math import factorial, comb\n",
    "import random\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# 1. Load data, split train/test sets, and extract required channels\n",
    "# ===============================\n",
    "out_dir = './dataset'\n",
    "method = 0\n",
    "model_dir = './model_standard/'\n",
    "metric_dir = './metrics_mean/'\n",
    "pred_dir = './pred_mean/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASH7aeMZyLVH"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1463,
     "status": "ok",
     "timestamp": 1749570078265,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "h1FklO-0eWL8"
   },
   "outputs": [],
   "source": [
    "def visualize_results_jet_contour(x_data, y_data, pred_data, mask_data, sample_index=0, num_samples=8,\n",
    "                      arrow_scale=15, arrow_width=0.008, arrow_headwidth=4, arrow_headlength=5,\n",
    "                      skip=1, hspace=0.1, wspace=0.2):\n",
    "    \"\"\"\n",
    "    Visualize U-Net results with vector fields showing velocity direction and magnitude, including error analysis.\n",
    "\n",
    "    Parameters:\n",
    "      x_data: Input data (N, 15, 15, 3)\n",
    "      y_data: Ground truth (N, 15, 15, 2)\n",
    "      pred_data: U-Net prediction results (N, 15, 15, 2)\n",
    "      mask_data: Sensor mask data (N, 15, 15, 3)\n",
    "      sample_index: Starting sample index\n",
    "      num_samples: Number of samples to visualize (default: 8)\n",
    "      arrow_scale: Scale parameter for quiver, smaller value gives larger arrows (default: 15)\n",
    "      arrow_width: Width of arrow shaft (default: 0.008)\n",
    "      arrow_headwidth: Width of arrow head as multiple of shaft width (default: 4)\n",
    "      arrow_headlength: Length of arrow head as multiple of shaft width (default: 5)\n",
    "      skip: Number of grid points to skip for clearer visualization (default: 1)\n",
    "      hspace: Height space between subplot rows (default: 0.1)\n",
    "      wspace: Width space between subplot columns (default: 0.2)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    # Ensure we don't exceed dataset size\n",
    "    max_samples = min(num_samples, len(x_data) - sample_index)\n",
    "\n",
    "    # Create figure with 4 rows and num_samples columns\n",
    "    # Add gridspec_kw to control spacing between subplots\n",
    "    fig, axs = plt.subplots(3, max_samples, figsize=(max_samples*3, 12),\n",
    "                           gridspec_kw={'hspace': hspace, 'wspace': wspace})\n",
    "\n",
    "    # If only one sample, make axs 2D\n",
    "    if max_samples == 1:\n",
    "        axs = axs.reshape(3, 1)\n",
    "\n",
    "    # Use jet colormap as in MATLAB code\n",
    "    cmap = 'jet'\n",
    "    error_cmap = 'hot'\n",
    "\n",
    "    # Calculate max error across all samples for consistent color scale\n",
    "    error_max = 0\n",
    "    for i in range(max_samples):\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Calculate velocity vector error magnitude\n",
    "        u_true = y_data[idx, :, :, 0]\n",
    "        v_true = y_data[idx, :, :, 1]\n",
    "        u_pred = pred_data[idx, :, :, 0]\n",
    "        v_pred = pred_data[idx, :, :, 1]\n",
    "\n",
    "        error_magnitude = np.sqrt((u_pred - u_true)**2 + (v_pred - v_true)**2)\n",
    "        error_max = max(error_max, np.max(error_magnitude))\n",
    "\n",
    "    # Plot each sample\n",
    "    for i in range(max_samples):\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Get sensor positions\n",
    "        sensor_mask = mask_data[idx, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "\n",
    "        # Create grid\n",
    "        y, x = np.mgrid[0:x_data.shape[1], 0:x_data.shape[2]]\n",
    "\n",
    "        # Row 1: Interpolated inputs (Observation)\n",
    "        u_interp = x_data[idx, :, :, 0]\n",
    "        v_interp = x_data[idx, :, :, 1]\n",
    "\n",
    "        # Calculate velocity magnitude\n",
    "        magnitude_interp = np.sqrt(u_interp**2 + v_interp**2)\n",
    "\n",
    "        # Use same max value for all velocity fields to normalize\n",
    "        max_magnitude = max(np.max(magnitude_interp),\n",
    "                            np.max(np.sqrt(pred_data[idx,:,:,0]**2 + pred_data[idx,:,:,1]**2)),\n",
    "                            np.max(np.sqrt(y_data[idx,:,:,0]**2 + y_data[idx,:,:,1]**2)))\n",
    "\n",
    "        # Plot vector field\n",
    "        axs[0, i].set_title(f'Sample {idx}')\n",
    "        norm = Normalize(vmin=0, vmax=max_magnitude)\n",
    "\n",
    "        # Add contour plot\n",
    "        contour = axs[0, i].contourf(x, y, magnitude_interp,\n",
    "                                    levels=15, cmap=cmap, norm=norm, alpha=0.7)\n",
    "\n",
    "        # quiver = axs[0, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "        #                  u_interp[::skip, ::skip], v_interp[::skip, ::skip],\n",
    "        #                  magnitude_interp[::skip, ::skip],\n",
    "        #                  cmap=cmap, norm=norm,\n",
    "        #                  scale=arrow_scale,\n",
    "        #                  width=arrow_width,\n",
    "        #                  headwidth=arrow_headwidth,\n",
    "        #                  headlength=arrow_headlength)\n",
    "        quiver = axs[0, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                  u_interp[::skip, ::skip], v_interp[::skip, ::skip],\n",
    "                  scale=arrow_scale,\n",
    "                  width=arrow_width,\n",
    "                  headwidth=arrow_headwidth,\n",
    "                  headlength=arrow_headlength,\n",
    "                  color='black')\n",
    "\n",
    "        if i == 0:\n",
    "            axs[0, i].set_ylabel('Interpolated Input', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        # Plot sensor positions\n",
    "        axs[0, i].scatter(sensor_positions[1], sensor_positions[0], c='black', s=10, marker='o')\n",
    "        axs[0, i].set_aspect('equal')\n",
    "        axs[0, i].set_xticks([])\n",
    "        axs[0, i].set_yticks([])\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(quiver, ax=axs[0, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        u_pred = pred_data[idx, :, :, 0]\n",
    "        v_pred = pred_data[idx, :, :, 1]\n",
    "        magnitude_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "\n",
    "        # Add contour plot\n",
    "        contour = axs[1, i].contourf(x, y, magnitude_pred,\n",
    "                                    levels=15, cmap=cmap, norm=norm, alpha=0.7)\n",
    "\n",
    "        # quiver = axs[1, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "        #                  u_pred[::skip, ::skip], v_pred[::skip, ::skip],\n",
    "        #                  magnitude_pred[::skip, ::skip],\n",
    "        #                  cmap=cmap, norm=norm,\n",
    "        #                  scale=arrow_scale,\n",
    "        #                  width=arrow_width,\n",
    "        #                  headwidth=arrow_headwidth,\n",
    "        #                  headlength=arrow_headlength)\n",
    "        quiver = axs[1, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                  u_pred[::skip, ::skip], v_pred[::skip, ::skip],\n",
    "                  scale=arrow_scale,\n",
    "                  width=arrow_width,\n",
    "                  headwidth=arrow_headwidth,\n",
    "                  headlength=arrow_headlength,\n",
    "                  color='black')\n",
    "\n",
    "        if i == 0:\n",
    "            axs[1, i].set_ylabel('Prediction', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        axs[1, i].set_aspect('equal')\n",
    "        axs[1, i].set_xticks([])\n",
    "        axs[1, i].set_yticks([])\n",
    "        cbar = fig.colorbar(quiver, ax=axs[1, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 3: Ground truth\n",
    "        u_true = y_data[idx, :, :, 0]\n",
    "        v_true = y_data[idx, :, :, 1]\n",
    "        magnitude_true = np.sqrt(u_true**2 + v_true**2)\n",
    "\n",
    "        # Add contour plot\n",
    "        contour = axs[2, i].contourf(x, y, magnitude_true,\n",
    "                                    levels=15, cmap=cmap, norm=norm, alpha=0.7)\n",
    "\n",
    "        # quiver = axs[2, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "        #                  u_true[::skip, ::skip], v_true[::skip, ::skip],\n",
    "        #                  magnitude_true[::skip, ::skip],\n",
    "        #                  cmap=cmap, norm=norm,\n",
    "        #                  scale=arrow_scale,\n",
    "        #                  width=arrow_width,\n",
    "        #                  headwidth=arrow_headwidth,\n",
    "        #                  headlength=arrow_headlength)\n",
    "        quiver = axs[2, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                  u_true[::skip, ::skip], v_true[::skip, ::skip],\n",
    "                  scale=arrow_scale,\n",
    "                  width=arrow_width,\n",
    "                  headwidth=arrow_headwidth,\n",
    "                  headlength=arrow_headlength,\n",
    "                  color='black')\n",
    "\n",
    "        if i == 0:\n",
    "            axs[2, i].set_ylabel('Ground Truth', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        axs[2, i].set_aspect('equal')\n",
    "        axs[2, i].set_xticks([])\n",
    "        axs[2, i].set_yticks([])\n",
    "        cbar = fig.colorbar(quiver, ax=axs[2, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 4: Error visualization\n",
    "        # error_magnitude = np.sqrt((u_pred - u_true)**2 + (v_pred - v_true)**2)\n",
    "        # im4 = axs[3, i].imshow(error_magnitude, cmap=error_cmap, vmin=0, vmax=error_max)\n",
    "\n",
    "        # if i == 0:\n",
    "        #     axs[3, i].set_ylabel('Error', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        # axs[3, i].set_aspect('equal')\n",
    "        # axs[3, i].set_xticks([])\n",
    "        # axs[3, i].set_yticks([])\n",
    "        # cbar = fig.colorbar(im4, ax=axs[3, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Error Magnitude')\n",
    "\n",
    "    # Adjust the layout (alternative to tight_layout which sometimes doesn't work well with colorbar)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1749570078851,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "pXVZfUbodSF9"
   },
   "outputs": [],
   "source": [
    "def visualize_results_jet(x_data, y_data, pred_data, mask_data, sample_index=0, num_samples=8,\n",
    "                      arrow_scale=15, arrow_width=0.008, arrow_headwidth=4, arrow_headlength=5,\n",
    "                      skip=1, hspace=0.1, wspace=0.2):\n",
    "    \"\"\"\n",
    "    Visualize U-Net results with vector fields showing velocity direction and magnitude, including error analysis.\n",
    "\n",
    "    Parameters:\n",
    "      x_data: Input data (N, 15, 15, 3)\n",
    "      y_data: Ground truth (N, 15, 15, 2)\n",
    "      pred_data: U-Net prediction results (N, 15, 15, 2)\n",
    "      mask_data: Sensor mask data (N, 15, 15, 3)\n",
    "      sample_index: Starting sample index\n",
    "      num_samples: Number of samples to visualize (default: 8)\n",
    "      arrow_scale: Scale parameter for quiver, smaller value gives larger arrows (default: 15)\n",
    "      arrow_width: Width of arrow shaft (default: 0.008)\n",
    "      arrow_headwidth: Width of arrow head as multiple of shaft width (default: 4)\n",
    "      arrow_headlength: Length of arrow head as multiple of shaft width (default: 5)\n",
    "      skip: Number of grid points to skip for clearer visualization (default: 1)\n",
    "      hspace: Height space between subplot rows (default: 0.1)\n",
    "      wspace: Width space between subplot columns (default: 0.2)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    # Ensure we don't exceed dataset size\n",
    "    max_samples = min(num_samples, len(x_data) - sample_index)\n",
    "\n",
    "    # Create figure with 4 rows and num_samples columns\n",
    "    # Add gridspec_kw to control spacing between subplots\n",
    "    fig, axs = plt.subplots(3, max_samples, figsize=(max_samples*3, 12),\n",
    "                           gridspec_kw={'hspace': hspace, 'wspace': wspace})\n",
    "\n",
    "    # If only one sample, make axs 2D\n",
    "    if max_samples == 1:\n",
    "        axs = axs.reshape(3, 1)\n",
    "\n",
    "    # Use jet colormap as in MATLAB code\n",
    "    cmap = 'jet'\n",
    "    error_cmap = 'hot'\n",
    "\n",
    "    # Calculate max error across all samples for consistent color scale\n",
    "    error_max = 0\n",
    "    for i in range(max_samples):\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Calculate velocity vector error magnitude\n",
    "        u_true = y_data[idx, :, :, 0]\n",
    "        v_true = y_data[idx, :, :, 1]\n",
    "        u_pred = pred_data[idx, :, :, 0]\n",
    "        v_pred = pred_data[idx, :, :, 1]\n",
    "\n",
    "        error_magnitude = np.sqrt((u_pred - u_true)**2 + (v_pred - v_true)**2)\n",
    "        error_max = max(error_max, np.max(error_magnitude))\n",
    "\n",
    "    # Plot each sample\n",
    "    for i in range(max_samples):\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Get sensor positions\n",
    "        sensor_mask = mask_data[idx, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "\n",
    "        # Create grid\n",
    "        y, x = np.mgrid[0:x_data.shape[1], 0:x_data.shape[2]]\n",
    "\n",
    "        # Row 1: Interpolated inputs (Observation)\n",
    "        u_interp = x_data[idx, :, :, 0]\n",
    "        v_interp = x_data[idx, :, :, 1]\n",
    "\n",
    "        # Calculate velocity magnitude\n",
    "        magnitude_interp = np.sqrt(u_interp**2 + v_interp**2)\n",
    "\n",
    "        # Use same max value for all velocity fields to normalize\n",
    "        max_magnitude = max(np.max(magnitude_interp),\n",
    "                            np.max(np.sqrt(pred_data[idx,:,:,0]**2 + pred_data[idx,:,:,1]**2)),\n",
    "                            np.max(np.sqrt(y_data[idx,:,:,0]**2 + y_data[idx,:,:,1]**2)))\n",
    "\n",
    "        # Plot vector field\n",
    "        axs[0, i].set_title(f'Sample {idx}')\n",
    "        norm = Normalize(vmin=0, vmax=max_magnitude)\n",
    "\n",
    "        quiver = axs[0, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                         u_interp[::skip, ::skip], v_interp[::skip, ::skip],\n",
    "                         magnitude_interp[::skip, ::skip],\n",
    "                         cmap=cmap, norm=norm,\n",
    "                         scale=arrow_scale,\n",
    "                         width=arrow_width,\n",
    "                         headwidth=arrow_headwidth,\n",
    "                         headlength=arrow_headlength)\n",
    "\n",
    "        if i == 0:\n",
    "            axs[0, i].set_ylabel('Interpolated Input', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        # Plot sensor positions\n",
    "        axs[0, i].scatter(sensor_positions[1], sensor_positions[0], c='black', s=10, marker='o')\n",
    "        axs[0, i].set_aspect('equal')\n",
    "        axs[0, i].set_xticks([])\n",
    "        axs[0, i].set_yticks([])\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(quiver, ax=axs[0, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        u_pred = pred_data[idx, :, :, 0]\n",
    "        v_pred = pred_data[idx, :, :, 1]\n",
    "        magnitude_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "\n",
    "        quiver = axs[1, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                         u_pred[::skip, ::skip], v_pred[::skip, ::skip],\n",
    "                         magnitude_pred[::skip, ::skip],\n",
    "                         cmap=cmap, norm=norm,\n",
    "                         scale=arrow_scale,\n",
    "                         width=arrow_width,\n",
    "                         headwidth=arrow_headwidth,\n",
    "                         headlength=arrow_headlength)\n",
    "\n",
    "        if i == 0:\n",
    "            axs[1, i].set_ylabel('Prediction', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        axs[1, i].set_aspect('equal')\n",
    "        axs[1, i].set_xticks([])\n",
    "        axs[1, i].set_yticks([])\n",
    "        cbar = fig.colorbar(quiver, ax=axs[1, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 3: Ground truth\n",
    "        u_true = y_data[idx, :, :, 0]\n",
    "        v_true = y_data[idx, :, :, 1]\n",
    "        magnitude_true = np.sqrt(u_true**2 + v_true**2)\n",
    "\n",
    "        quiver = axs[2, i].quiver(x[::skip, ::skip], y[::skip, ::skip],\n",
    "                         u_true[::skip, ::skip], v_true[::skip, ::skip],\n",
    "                         magnitude_true[::skip, ::skip],\n",
    "                         cmap=cmap, norm=norm,\n",
    "                         scale=arrow_scale,\n",
    "                         width=arrow_width,\n",
    "                         headwidth=arrow_headwidth,\n",
    "                         headlength=arrow_headlength)\n",
    "\n",
    "        if i == 0:\n",
    "            axs[2, i].set_ylabel('Ground Truth', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        axs[2, i].set_aspect('equal')\n",
    "        axs[2, i].set_xticks([])\n",
    "        axs[2, i].set_yticks([])\n",
    "        cbar = fig.colorbar(quiver, ax=axs[2, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Velocity Magnitude')\n",
    "\n",
    "        # Row 4: Error visualization\n",
    "        # error_magnitude = np.sqrt((u_pred - u_true)**2 + (v_pred - v_true)**2)\n",
    "        # im4 = axs[3, i].imshow(error_magnitude, cmap=error_cmap, vmin=0, vmax=error_max)\n",
    "\n",
    "        # if i == 0:\n",
    "        #     axs[3, i].set_ylabel('Error', rotation=90, labelpad=15, va='center')\n",
    "\n",
    "        # axs[3, i].set_aspect('equal')\n",
    "        # axs[3, i].set_xticks([])\n",
    "        # axs[3, i].set_yticks([])\n",
    "        # cbar = fig.colorbar(im4, ax=axs[3, i], fraction=0.046, pad=0.04)\n",
    "        #cbar.set_label('Error Magnitude')\n",
    "\n",
    "    # Adjust the layout (alternative to tight_layout which sometimes doesn't work well with colorbar)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1749570078853,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "V5vP19X7ykmw"
   },
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "@register_keras_serializable()\n",
    "def weighted_vector_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Robust loss function optimized for wind direction and high wind speed, with enhanced numerical stability\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    # Set larger numerical stability constant\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    # Extract U and V components\n",
    "    true_u = y_true[..., 0]\n",
    "    true_v = y_true[..., 1]\n",
    "    pred_u = y_pred[..., 0]\n",
    "    pred_v = y_pred[..., 1]\n",
    "\n",
    "    # Calculate wind speed magnitude - added stronger numerical protection\n",
    "    true_magnitude = tf.sqrt(tf.square(true_u) + tf.square(true_v) + epsilon)\n",
    "    pred_magnitude = tf.sqrt(tf.square(pred_u) + tf.square(pred_v) + epsilon)\n",
    "\n",
    "    # 1. MSE loss for wind speed magnitude\n",
    "    magnitude_loss = tf.square(true_magnitude - pred_magnitude)\n",
    "\n",
    "    # 2. Wind speed weight - use bounded nonlinear function instead of exponential\n",
    "    # Calculate relative wind speed magnitude\n",
    "    max_magnitude = tf.reduce_max(true_magnitude, axis=[1, 2], keepdims=True)\n",
    "    min_magnitude = tf.reduce_min(true_magnitude, axis=[1, 2], keepdims=True)\n",
    "\n",
    "    # Ensure denominator is not zero and normalize to [0,1] range\n",
    "    magnitude_range = tf.maximum(max_magnitude - min_magnitude, epsilon)\n",
    "    normalized_magnitude = tf.clip_by_value(\n",
    "        (true_magnitude - min_magnitude) / magnitude_range,\n",
    "        0.0,\n",
    "        1.0\n",
    "    )\n",
    "\n",
    "    # Use more stable bounded function: 1 + 5*x^2, result range is [1,6]\n",
    "    # This is safer than exponential function but still gives higher weight to high wind speed regions\n",
    "    magnitude_weights = 1.0 + 5.0 * tf.square(normalized_magnitude)\n",
    "\n",
    "    # Check and report any potential NaN\n",
    "    magnitude_weights = tf.debugging.check_numerics(\n",
    "        magnitude_weights,\n",
    "        \"NaN/Inf found in magnitude_weights\"\n",
    "    )\n",
    "\n",
    "    # 3. Wind direction loss - improve vector normalization stability\n",
    "    # Ensure denominator is large enough\n",
    "    true_norm = tf.maximum(true_magnitude, epsilon)\n",
    "    pred_norm = tf.maximum(pred_magnitude, epsilon)\n",
    "\n",
    "    # Normalize vectors\n",
    "    true_u_norm = true_u / true_norm\n",
    "    true_v_norm = true_v / true_norm\n",
    "    pred_u_norm = pred_u / pred_norm\n",
    "    pred_v_norm = pred_v / pred_norm\n",
    "\n",
    "    # Calculate cosine similarity and enforce range limits\n",
    "    cos_similarity = true_u_norm * pred_u_norm + true_v_norm * pred_v_norm\n",
    "    cos_similarity_safe = tf.clip_by_value(cos_similarity, -1.0 + epsilon, 1.0 - epsilon)\n",
    "\n",
    "    # 4. Specifically penalize direction errors\n",
    "    # Method 1: Use 1-cos(theta) directly as direction loss, range is [0,2]\n",
    "    # This is more stable than acos and still penalizes opposite directions\n",
    "    direction_loss_simple = 1.0 - cos_similarity_safe\n",
    "\n",
    "    # Method 2: Additional weighting for negative directions (>90 degrees), safer implementation\n",
    "    # When cos_similarity<0 (angle>90 degrees), add additional penalty\n",
    "    direction_loss_weighted = tf.where(\n",
    "        cos_similarity_safe < 0,\n",
    "        2.0 - 2.0 * cos_similarity_safe,  # For [0,-1] range, map to [2,4]\n",
    "        1.0 - cos_similarity_safe         # For [1,0] range, map to [0,1]\n",
    "    )\n",
    "\n",
    "    # Use safer weighting scheme\n",
    "    direction_loss = direction_loss_weighted\n",
    "\n",
    "    # 5. Wind direction weight - the higher the wind speed, the more important the direction\n",
    "    # Use simple linear mapping, range is [0.3,1.0]\n",
    "    direction_weight = 0.3 + 0.7 * normalized_magnitude\n",
    "\n",
    "    # 6. Combined loss - use separate weight control instead of multiplication to avoid weight explosion\n",
    "    weighted_magnitude_loss = magnitude_weights * magnitude_loss\n",
    "    weighted_direction_loss = direction_weight * direction_loss\n",
    "\n",
    "    # Set component weights\n",
    "    lambda_magnitude = 1.0\n",
    "    lambda_direction = 2.0  # Increase overall weight of direction loss\n",
    "\n",
    "    # Calculate final loss and add monitoring\n",
    "    final_magnitude_loss = tf.reduce_mean(weighted_magnitude_loss)\n",
    "    final_direction_loss = tf.reduce_mean(weighted_direction_loss)\n",
    "\n",
    "    # Verify results have no NaN\n",
    "    final_magnitude_loss = tf.debugging.check_numerics(\n",
    "        final_magnitude_loss,\n",
    "        \"NaN/Inf found in final_magnitude_loss\"\n",
    "    )\n",
    "    final_direction_loss = tf.debugging.check_numerics(\n",
    "        final_direction_loss,\n",
    "        \"NaN/Inf found in final_direction_loss\"\n",
    "    )\n",
    "\n",
    "    total_loss = lambda_magnitude * final_magnitude_loss + lambda_direction * final_direction_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1749570078902,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "mk8hdWCposuD"
   },
   "outputs": [],
   "source": [
    "def apply_kriging_reconstruction(x_test, y_test, method='ordinary'):\n",
    "    \"\"\"\n",
    "    Reconstruct flow field using Kriging method, optimize correlation length using experimental variogram fitting\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    from pykrige.ok import OrdinaryKriging\n",
    "    from pykrige.uk import UniversalKriging\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "    print(f\"Reconstructing flow field using {method} Kriging method...\")\n",
    "\n",
    "    # Get grid dimensions\n",
    "    h, w = x_test.shape[1:3]\n",
    "    grid_x = np.arange(0, w, 1.0)\n",
    "    grid_y = np.arange(0, h, 1.0)\n",
    "\n",
    "    # Define variogram models to test\n",
    "    variogram_models = ['gaussian', 'spherical', 'exponential']\n",
    "\n",
    "    # Define theoretical variogram models\n",
    "    def gaussian_variogram(h, range_param, sill, nugget):\n",
    "        # Gaussian variogram model\n",
    "        return nugget + sill * (1 - np.exp(-(h**2) / (range_param**2)))\n",
    "\n",
    "    def spherical_variogram(h, range_param, sill, nugget):\n",
    "        # Spherical variogram model\n",
    "        result = np.zeros_like(h, dtype=float)\n",
    "        mask = h <= range_param\n",
    "        result[mask] = nugget + sill * ((3*h[mask])/(2*range_param) - (h[mask]**3)/(2*range_param**3))\n",
    "        result[~mask] = nugget + sill\n",
    "        return result\n",
    "\n",
    "    def exponential_variogram(h, range_param, sill, nugget):\n",
    "        # Exponential variogram model\n",
    "        return nugget + sill * (1 - np.exp(-h/range_param))\n",
    "\n",
    "                    # Calculate experimental variogram\n",
    "    def calculate_experimental_variogram(x, y, values, n_lags=20, max_dist=None):\n",
    "        # Combine coordinates\n",
    "        coords = np.column_stack((x, y))\n",
    "\n",
    "        # Calculate distances between point pairs\n",
    "        distances = squareform(pdist(coords, 'euclidean'))\n",
    "\n",
    "        # Set maximum distance and lag width\n",
    "        if max_dist is None:\n",
    "            max_dist = np.max(distances) / 2\n",
    "\n",
    "        lag_width = max_dist / n_lags\n",
    "        lags = np.arange(lag_width/2, max_dist, lag_width)\n",
    "\n",
    "        # Initialize variogram values and point pair counts\n",
    "        gamma = np.zeros(len(lags))\n",
    "        counts = np.zeros(len(lags))\n",
    "\n",
    "        # Calculate semivariance\n",
    "        n = len(values)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                dist = distances[i, j]\n",
    "                if dist <= max_dist:\n",
    "                    # Determine lag index\n",
    "                    lag_idx = int(dist / lag_width)\n",
    "                    if lag_idx < len(lags):\n",
    "                        # Calculate squared value difference\n",
    "                        val_diff = (values[i] - values[j])**2\n",
    "                        gamma[lag_idx] += val_diff\n",
    "                        counts[lag_idx] += 1\n",
    "\n",
    "        # Calculate average semivariance\n",
    "        valid_idx = counts > 0\n",
    "        gamma[valid_idx] /= (2 * counts[valid_idx])\n",
    "\n",
    "        return lags[valid_idx], gamma[valid_idx]\n",
    "\n",
    "    # Fit theoretical variogram\n",
    "    def fit_variogram_model(lags, gamma, model_type):\n",
    "        try:\n",
    "            if model_type == 'gaussian':\n",
    "                model_func = gaussian_variogram\n",
    "            elif model_type == 'spherical':\n",
    "                model_func = spherical_variogram\n",
    "            else:  # exponential\n",
    "                model_func = exponential_variogram\n",
    "\n",
    "            # Initial parameter guess: range, sill, nugget\n",
    "            initial_guess = [np.mean(lags), np.max(gamma), 0.0]\n",
    "\n",
    "            # Fit variogram\n",
    "            params, _ = curve_fit(model_func, lags, gamma, p0=initial_guess,\n",
    "                                 bounds=([0.01, 0.01, 0], [np.max(lags)*2, np.max(gamma)*2, np.max(gamma)*0.5]),\n",
    "                                 maxfev=1000)\n",
    "\n",
    "            # Calculate fitting error\n",
    "            fitted_values = model_func(lags, *params)\n",
    "            mse = np.mean((gamma - fitted_values)**2)\n",
    "\n",
    "            return params, mse\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fit {model_type} model: {e}\")\n",
    "            return [3.0, 1.0, 0.0], float('inf')  # Return default parameters and infinite error\n",
    "\n",
    "    # Sample selection\n",
    "    sample_size = min(100, len(x_test))\n",
    "    sample_indices = np.random.choice(len(x_test), sample_size, replace=False)\n",
    "\n",
    "    print(\"Finding optimal parameters using experimental variogram fitting method...\")\n",
    "\n",
    "    best_params_u = {'model': 'gaussian', 'range': 3.0, 'sill': 1.0, 'nugget': 0.0}\n",
    "    best_params_v = {'model': 'gaussian', 'range': 3.0, 'sill': 1.0, 'nugget': 0.0}\n",
    "    best_mse_u = float('inf')\n",
    "    best_mse_v = float('inf')\n",
    "\n",
    "    # Calculate variogram and fit for each sample\n",
    "    for i in tqdm(sample_indices, desc=\"Variogram fitting progress\"):\n",
    "        sensor_mask = x_test[i, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "        x_coords = sensor_positions[1]\n",
    "        y_coords = sensor_positions[0]\n",
    "\n",
    "        if len(x_coords) < 10:  # Ensure sufficient points for variogram calculation\n",
    "            continue\n",
    "\n",
    "        for c in range(2):\n",
    "            values = y_test[i, y_coords, x_coords, c]\n",
    "\n",
    "            try:\n",
    "                # Calculate experimental variogram\n",
    "                lags, gamma = calculate_experimental_variogram(x_coords, y_coords, values)\n",
    "\n",
    "                if len(lags) < 3:  # Ensure sufficient point pairs for fitting\n",
    "                    continue\n",
    "\n",
    "                # Test different variogram models\n",
    "                for model_type in variogram_models:\n",
    "                    # Fit variogram model\n",
    "                    params, mse = fit_variogram_model(lags, gamma, model_type)\n",
    "\n",
    "                    # Update best parameters\n",
    "                    if c == 0 and mse < best_mse_u:\n",
    "                        best_mse_u = mse\n",
    "                        best_params_u = {'model': model_type, 'range': params[0], 'sill': params[1], 'nugget': params[2]}\n",
    "                    elif c == 1 and mse < best_mse_v:\n",
    "                        best_mse_v = mse\n",
    "                        best_params_v = {'model': model_type, 'range': params[0], 'sill': params[1], 'nugget': params[2]}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Variogram calculation failed for sample {i}, component {c}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Optimal variogram parameters - U component: {best_params_u}\")\n",
    "    print(f\"Optimal variogram parameters - V component: {best_params_v}\")\n",
    "\n",
    "    # Validate optimal parameters found using cross-validation\n",
    "    print(\"Validating optimal parameters using cross-validation...\")\n",
    "\n",
    "    validation_size = min(30, len(x_test))\n",
    "    validation_indices = np.random.choice([i for i in range(len(x_test)) if i not in sample_indices],\n",
    "                                         validation_size, replace=False)\n",
    "\n",
    "    rmse_u = 0\n",
    "    rmse_v = 0\n",
    "    valid_count = 0\n",
    "\n",
    "    for i in tqdm(validation_indices, desc=\"Cross-validation progress\"):\n",
    "        sensor_mask = x_test[i, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "        x_coords = sensor_positions[1]\n",
    "        y_coords = sensor_positions[0]\n",
    "\n",
    "        if len(x_coords) < 5:\n",
    "            continue\n",
    "\n",
    "        # Randomly select 30% of points as test points\n",
    "        test_points = max(int(len(x_coords) * 0.3), 1)\n",
    "        test_idx = np.random.choice(len(x_coords), test_points, replace=False)\n",
    "        train_idx = np.array([j for j in range(len(x_coords)) if j not in test_idx])\n",
    "\n",
    "        for c in range(2):\n",
    "            values = y_test[i, y_coords, x_coords, c]\n",
    "            train_values = values[train_idx]\n",
    "            test_values = values[test_idx]\n",
    "\n",
    "            current_params = best_params_u if c == 0 else best_params_v\n",
    "\n",
    "            try:\n",
    "                if method.lower() == 'ordinary':\n",
    "                    krig = OrdinaryKriging(\n",
    "                        x_coords[train_idx], y_coords[train_idx], train_values,\n",
    "                        variogram_model=current_params['model'],\n",
    "                        variogram_parameters={\n",
    "                            'range': current_params['range'],\n",
    "                            'sill': current_params['sill'],\n",
    "                            'nugget': current_params['nugget']\n",
    "                        },\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "                else:\n",
    "                    krig = UniversalKriging(\n",
    "                        x_coords[train_idx], y_coords[train_idx], train_values,\n",
    "                        variogram_model=current_params['model'],\n",
    "                        variogram_parameters={\n",
    "                            'range': current_params['range'],\n",
    "                            'sill': current_params['sill'],\n",
    "                            'nugget': current_params['nugget']\n",
    "                        },\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "\n",
    "                # Predict on test points\n",
    "                test_coords = np.array([x_coords[test_idx], y_coords[test_idx]]).T\n",
    "                pred_values, _ = krig.execute('points', test_coords[:, 0], test_coords[:, 1])\n",
    "\n",
    "                # Calculate RMSE\n",
    "                error = np.sqrt(np.mean((pred_values - test_values)**2))\n",
    "\n",
    "                if c == 0:\n",
    "                    rmse_u += error\n",
    "                else:\n",
    "                    rmse_v += error\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        valid_count += 1\n",
    "\n",
    "    if valid_count > 0:\n",
    "        rmse_u /= valid_count\n",
    "        rmse_v /= valid_count\n",
    "        print(f\"Cross-validation RMSE - U: {rmse_u:.6f}, V: {rmse_v:.6f}\")\n",
    "\n",
    "    # Predict all samples using optimal parameters\n",
    "    print(\"Predicting using optimal parameters...\")\n",
    "    pred_kriging = np.zeros_like(y_test)\n",
    "\n",
    "    for i in tqdm(range(len(x_test)), desc=\"Kriging reconstruction progress\"):\n",
    "        sensor_mask = x_test[i, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "        x_coords = sensor_positions[1]\n",
    "        y_coords = sensor_positions[0]\n",
    "\n",
    "        for c in range(2):\n",
    "            values = y_test[i, y_coords, x_coords, c]\n",
    "\n",
    "            if len(x_coords) < 3:\n",
    "                pred_kriging[i, :, :, c] = x_test[i, :, :, c]\n",
    "                continue\n",
    "\n",
    "            current_params = best_params_u if c == 0 else best_params_v\n",
    "\n",
    "            try:\n",
    "                if method.lower() == 'ordinary':\n",
    "                    krig = OrdinaryKriging(\n",
    "                        x_coords, y_coords, values,\n",
    "                        variogram_model=current_params['model'],\n",
    "                        variogram_parameters={\n",
    "                            'range': current_params['range'],\n",
    "                            'sill': current_params['sill'],\n",
    "                            'nugget': current_params['nugget']\n",
    "                        },\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "                else:\n",
    "                    krig = UniversalKriging(\n",
    "                        x_coords, y_coords, values,\n",
    "                        variogram_model=current_params['model'],\n",
    "                        variogram_parameters={\n",
    "                            'range': current_params['range'],\n",
    "                            'sill': current_params['sill'],\n",
    "                            'nugget': current_params['nugget']\n",
    "                        },\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "\n",
    "                z, ss = krig.execute('grid', grid_x, grid_y)\n",
    "                pred_kriging[i, :, :, c] = z\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Kriging interpolation failed for sample {i}, component {c}: {e}\")\n",
    "                pred_kriging[i, :, :, c] = x_test[i, :, :, c]\n",
    "\n",
    "    return pred_kriging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_my_data_opt(sensor_num, method, data_dir='./optimal_dataset'):\n",
    "    \"\"\"\n",
    "    Load data generated based on optimal sensor positions\n",
    "    \n",
    "    Parameters:\n",
    "        sensor_num: Number of sensors (5, 10, 15, 20, 25, 30)\n",
    "        method: Method number (0 or 1)\n",
    "        data_dir: Data file directory\n",
    "    \n",
    "    Returns:\n",
    "        Same output format as original load_my_data function\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_file(csv_name, data_type):\n",
    "        \"\"\"Helper function: load specific file\"\"\"\n",
    "        filename = f\"{csv_name}_method{method}_sensor{sensor_num}_{data_type}.npy\"\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"File does not exist: {filepath}\")\n",
    "        \n",
    "        return np.load(filepath, mmap_mode='r')\n",
    "    \n",
    "    print(f\"Loading optimal data: Method {method}, Sensor_num {sensor_num}\")\n",
    "    \n",
    "    try:\n",
    "        if method == 0:\n",
    "            # Method 0: Training uses 3 from 0deg, testing uses 2 from 22deg + 3 from 45deg\n",
    "            \n",
    "            # === Training data (3 from 0deg) ===\n",
    "            x1_data_0 = load_file('0deg_1', 'X')\n",
    "            y1_data_0 = load_file('0deg_1', 'y')\n",
    "            x2_data_0 = load_file('0deg_2', 'X')\n",
    "            y2_data_0 = load_file('0deg_2', 'y')\n",
    "            x3_data_0 = load_file('0deg_3', 'X')\n",
    "            y3_data_0 = load_file('0deg_3', 'y')\n",
    "            \n",
    "            # === Test data (2 from 22deg + 3 from 45deg) ===\n",
    "            x1_data_22 = load_file('22deg_1', 'X')\n",
    "            y1_data_22 = load_file('22deg_1', 'y')\n",
    "            x2_data_22 = load_file('22deg_2', 'X')\n",
    "            y2_data_22 = load_file('22deg_2', 'y')\n",
    "            \n",
    "            x1_data_45 = load_file('45deg_1', 'X')\n",
    "            y1_data_45 = load_file('45deg_1', 'y')\n",
    "            x2_data_45 = load_file('45deg_2', 'X')\n",
    "            y2_data_45 = load_file('45deg_2', 'y')\n",
    "            x3_data_45 = load_file('45deg_3', 'X')\n",
    "            y3_data_45 = load_file('45deg_3', 'y')\n",
    "            \n",
    "            # === Perturbed test data ===\n",
    "            x1_perturbed_data_22 = load_file('22deg_1', 'X_perturbed')\n",
    "            x2_perturbed_data_22 = load_file('22deg_2', 'X_perturbed')\n",
    "            x1_perturbed_data_45 = load_file('45deg_1', 'X_perturbed')\n",
    "            x2_perturbed_data_45 = load_file('45deg_2', 'X_perturbed')\n",
    "            x3_perturbed_data_45 = load_file('45deg_3', 'X_perturbed')\n",
    "            \n",
    "            # Combine training data\n",
    "            x_data = np.concatenate([x1_data_0, x2_data_0, x3_data_0], axis=0)\n",
    "            y_data = np.concatenate([y1_data_0, y2_data_0, y3_data_0], axis=0)\n",
    "            \n",
    "            # Combine test data\n",
    "            x_test = np.concatenate([x1_data_22, x2_data_22, x1_data_45, x2_data_45, x3_data_45], axis=0)\n",
    "            x_perturbed_test = np.concatenate([x1_perturbed_data_22, x2_perturbed_data_22, \n",
    "                                             x1_perturbed_data_45, x2_perturbed_data_45, x3_perturbed_data_45], axis=0)\n",
    "            y_test = np.concatenate([y1_data_22, y2_data_22, y1_data_45, y2_data_45, y3_data_45], axis=0)\n",
    "            \n",
    "            print(f\"Method 0 - Training data: 0deg_1,2,3\")\n",
    "            print(f\"Method 0 - Test data: 22deg_1,2 + 45deg_1,2,3\")\n",
    "            \n",
    "        elif method == 1:\n",
    "            # Method 1: Training uses 0deg_1 + 22deg_1 + 45deg_1, testing uses the rest\n",
    "            \n",
    "            # === Training data (0deg_1, 22deg_1, 45deg_1) ===\n",
    "            x1_data_0 = load_file('0deg_1', 'X')\n",
    "            y1_data_0 = load_file('0deg_1', 'y')\n",
    "            x1_data_22 = load_file('22deg_1', 'X')\n",
    "            y1_data_22 = load_file('22deg_1', 'y')\n",
    "            x1_data_45 = load_file('45deg_1', 'X')\n",
    "            y1_data_45 = load_file('45deg_1', 'y')\n",
    "            \n",
    "            # === Test data (0deg_2,3 + 22deg_2 + 45deg_2,3) ===\n",
    "            x2_data_0 = load_file('0deg_2', 'X')\n",
    "            y2_data_0 = load_file('0deg_2', 'y')\n",
    "            x3_data_0 = load_file('0deg_3', 'X')\n",
    "            y3_data_0 = load_file('0deg_3', 'y')\n",
    "            x2_data_22 = load_file('22deg_2', 'X')\n",
    "            y2_data_22 = load_file('22deg_2', 'y')\n",
    "            x2_data_45 = load_file('45deg_2', 'X')\n",
    "            y2_data_45 = load_file('45deg_2', 'y')\n",
    "            x3_data_45 = load_file('45deg_3', 'X')\n",
    "            y3_data_45 = load_file('45deg_3', 'y')\n",
    "            \n",
    "            # === Perturbed test data ===\n",
    "            x2_perturbed_data_0 = load_file('0deg_2', 'X_perturbed')\n",
    "            x3_perturbed_data_0 = load_file('0deg_3', 'X_perturbed')\n",
    "            x2_perturbed_data_22 = load_file('22deg_2', 'X_perturbed')\n",
    "            x2_perturbed_data_45 = load_file('45deg_2', 'X_perturbed')\n",
    "            x3_perturbed_data_45 = load_file('45deg_3', 'X_perturbed')\n",
    "            \n",
    "            # Combine training data\n",
    "            x_data = np.concatenate([x1_data_0, x1_data_22, x1_data_45], axis=0)\n",
    "            y_data = np.concatenate([y1_data_0, y1_data_22, y1_data_45], axis=0)\n",
    "            \n",
    "            # Combine test data\n",
    "            x_test = np.concatenate([x2_data_0, x3_data_0, x2_data_22, x2_data_45, x3_data_45], axis=0)\n",
    "            x_perturbed_test = np.concatenate([x2_perturbed_data_0, x3_perturbed_data_0, \n",
    "                                             x2_perturbed_data_22, x2_perturbed_data_45, x3_perturbed_data_45], axis=0)\n",
    "            y_test = np.concatenate([y2_data_0, y3_data_0, y2_data_22, y2_data_45, y3_data_45], axis=0)\n",
    "            \n",
    "            print(f\"Method 1 - Training data: 0deg_1 + 22deg_1 + 45deg_1\")\n",
    "            print(f\"Method 1 - Test data: 0deg_2,3 + 22deg_2 + 45deg_2,3\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method value: {method}, only 0 or 1 supported\")\n",
    "        \n",
    "        # Data shape information\n",
    "        print(f\"Training data shape: X={x_data.shape}, y={y_data.shape}\")\n",
    "        print(f\"Test data shape: X={x_test.shape}, y={y_test.shape}\")\n",
    "        print(f\"Perturbed test data shape: X_perturbed={x_perturbed_test.shape}\")\n",
    "        \n",
    "        # Train-validation split\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Data scaling\n",
    "        x_train_scaled, _, _ = scale_data(x_train)\n",
    "        x_val_scaled, _, _ = scale_data(x_val)\n",
    "        x_test_scaled, _, _ = scale_data(x_test)\n",
    "        x_perturbed_test_scaled, _, _ = scale_data(x_perturbed_test)\n",
    "        y_train_scaled, _, _ = scale_data(y_train)\n",
    "        y_val_scaled, _, _ = scale_data(y_val)\n",
    "        y_test_scaled, min_vals, max_vals = scale_data(y_test)\n",
    "        \n",
    "        # Prepare output\n",
    "        x_train_input = x_train_scaled\n",
    "        y_train_output = y_train_scaled\n",
    "        x_val_input = x_val_scaled\n",
    "        y_val_output = y_val_scaled\n",
    "        x_test_input = x_test_scaled\n",
    "        x_perturbed_test_input = x_perturbed_test_scaled\n",
    "        y_test_output = y_test_scaled\n",
    "        \n",
    "        print(f\" Data loading completed!\")\n",
    "        \n",
    "        return (x_train_input, y_train_output, x_val_input, y_val_output, \n",
    "                x_test_input, x_perturbed_test_input, y_test_output, \n",
    "                min_vals, max_vals, y_test, x_test)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" File loading failed: {e}\")\n",
    "        print(f\"Please ensure batch_generate_data.py has been run to generate required data files\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\" Error occurred during data loading: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749570732253,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "Y4TQUXPEpTay"
   },
   "outputs": [],
   "source": [
    "def load_my_data(sensor_num, method):\n",
    "  x1_data_0 = np.load(f'{out_dir}/x1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y1_data_0 = np.load(f'{out_dir}/y1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_data_0 = np.load(f'{out_dir}/x2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y2_data_0 = np.load(f'{out_dir}/y2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_data_0 = np.load(f'{out_dir}/x3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y3_data_0 = np.load(f'{out_dir}/y3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_perturbed_data_0 = np.load(f'{out_dir}/x2_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_perturbed_data_0 = np.load(f'{out_dir}/x3_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "\n",
    "  x1_data_45 = np.load(f'{out_dir}/45deg_x1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y1_data_45 = np.load(f'{out_dir}/45deg_y1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_data_45 = np.load(f'{out_dir}/45deg_x2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y2_data_45 = np.load(f'{out_dir}/45deg_y2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_data_45 = np.load(f'{out_dir}/45deg_x3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y3_data_45 = np.load(f'{out_dir}/45deg_y3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x1_perturbed_data_45 = np.load(f'{out_dir}/45deg_x1_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_perturbed_data_45 = np.load(f'{out_dir}/45deg_x2_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_perturbed_data_45 = np.load(f'{out_dir}/45deg_x3_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "\n",
    "  x1_data_22 = np.load(f'{out_dir}/22deg_x1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y1_data_22 = np.load(f'{out_dir}/22deg_y1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_data_22 = np.load(f'{out_dir}/22deg_x2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y2_data_22 = np.load(f'{out_dir}/22deg_y2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x1_perturbed_data_22 = np.load(f'{out_dir}/22deg_x1_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_perturbed_data_22 = np.load(f'{out_dir}/22deg_x2_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "\n",
    "  if method == 0:\n",
    "    x_data = np.concatenate([x1_data_0, x2_data_0, x3_data_0], axis=0)\n",
    "    y_data = np.concatenate([y1_data_0, y2_data_0, y3_data_0], axis=0)\n",
    "    x_test = np.concatenate([x1_data_22, x2_data_22, x1_data_45, x2_data_45, x3_data_45], axis=0)\n",
    "    x_perturbed_test = np.concatenate([x1_perturbed_data_22, x2_perturbed_data_22, x1_perturbed_data_45, x2_perturbed_data_45, x3_perturbed_data_45], axis=0)\n",
    "    y_test = np.concatenate([y1_data_22, y2_data_22, y1_data_45, y2_data_45, y3_data_45], axis=0)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "    x_train_scaled,_,_ = scale_data(x_train)\n",
    "    x_val_scaled,_,_ = scale_data(x_val)\n",
    "    x_test_scaled,_,_ = scale_data(x_test)\n",
    "    x_perturbed_test_scaled,_,_ = scale_data(x_perturbed_test)\n",
    "    y_train_scaled,_,_ = scale_data(y_train)\n",
    "    y_val_scaled,_,_ = scale_data(y_val)\n",
    "    y_test_scaled,min_vals,max_vals = scale_data(y_test)\n",
    "    x_train_input = x_train_scaled#[:-10]  # shape: (N, 15, 15, 3)\n",
    "    y_train_output = y_train_scaled#[10:]  # shape: (N, 15, 15, 2)\n",
    "    x_val_input = x_val_scaled\n",
    "    y_val_output = y_val_scaled\n",
    "    x_test_input = x_test_scaled#[:-10]\n",
    "    x_perturbed_test_input = x_perturbed_test_scaled\n",
    "    y_test_output = y_test_scaled#[10:]\n",
    "  elif method == 1:\n",
    "    x_data = np.concatenate([x1_data_0, x1_data_22, x1_data_45], axis=0)\n",
    "    y_data = np.concatenate([y1_data_0, y1_data_22, y1_data_45], axis=0)\n",
    "    x_test = np.concatenate([x2_data_0, x3_data_0, x2_data_22, x2_data_45, x3_data_45], axis=0)\n",
    "    x_perturbed_test = np.concatenate([x2_perturbed_data_0, x3_perturbed_data_0, x2_perturbed_data_22, x2_perturbed_data_45, x3_perturbed_data_45], axis=0)\n",
    "    y_test = np.concatenate([y2_data_0, y3_data_0, y2_data_22, y2_data_45, y3_data_45], axis=0)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "    x_train_scaled,_,_ = scale_data(x_train)\n",
    "    x_val_scaled,_,_ = scale_data(x_val)\n",
    "    x_test_scaled,_,_ = scale_data(x_test)\n",
    "    x_perturbed_test_scaled,_,_ = scale_data(x_perturbed_test)\n",
    "    y_train_scaled,_,_ = scale_data(y_train)\n",
    "    y_val_scaled,_,_ = scale_data(y_val)\n",
    "    y_test_scaled,min_vals,max_vals = scale_data(y_test)\n",
    "    x_train_input = x_train_scaled#[:-10]  # shape: (N, 15, 15, 3)\n",
    "    y_train_output = y_train_scaled#[10:]  # shape: (N, 15, 15, 2)\n",
    "    x_val_input = x_val_scaled\n",
    "    y_val_output = y_val_scaled\n",
    "    x_test_input = x_test_scaled#[:-10]\n",
    "    x_perturbed_test_input = x_perturbed_test_scaled\n",
    "    y_test_output = y_test_scaled#[10:]\n",
    "\n",
    "  # x_data = np.concatenate([x1_data, x2_data], axis=0)\n",
    "  # y_data = np.concatenate([y1_data, y2_data], axis=0)\n",
    "  # x_test = x3_data\n",
    "  # x_perturbed_test = x3_perturbed_data\n",
    "  # y_test = y3_data\n",
    "  # # x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "  # x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "  # x_train_scaled,_,_ = scale_data(x_train)\n",
    "  # x_val_scaled,_,_ = scale_data(x_val)\n",
    "  # x_test_scaled,_,_ = scale_data(x_test)\n",
    "  # x_perturbed_test_scaled,_,_ = scale_data(x_perturbed_test)\n",
    "  # y_train_scaled,_,_ = scale_data(y_train)\n",
    "  # y_val_scaled,_,_ = scale_data(y_val)\n",
    "  # y_test_scaled,min_vals,max_vals = scale_data(y_test)\n",
    "  # x_train_input = x_train_scaled#[:-10]  # shape: (N, 15, 15, 3)\n",
    "  # y_train_output = y_train_scaled#[10:]  # shape: (N, 15, 15, 2)\n",
    "  # x_val_input = x_val_scaled\n",
    "  # y_val_output = y_val_scaled\n",
    "  # x_test_input = x_test_scaled#[:-10]\n",
    "  # x_perturbed_test_input = x_perturbed_test_scaled\n",
    "  # y_test_output = y_test_scaled#[10:]\n",
    "  return x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749570078990,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "E1fYIZhcvZSu"
   },
   "outputs": [],
   "source": [
    "def load_perturbed_data(sensor_num):\n",
    "  x1_data = np.load(f'{out_dir}/x1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y1_data = np.load(f'{out_dir}/y1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x2_data = np.load(f'{out_dir}/x2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y2_data = np.load(f'{out_dir}/y2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_data = np.load(f'{out_dir}/x3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  y3_data = np.load(f'{out_dir}/y3_data_{sensor_num}.npy', mmap_mode='r')\n",
    "  x3_perturbed_data = np.load(f'{out_dir}/x3_perturbed_data_{sensor_num}.npy', mmap_mode='r')\n",
    "\n",
    "  x_data = np.concatenate([x1_data, x2_data], axis=0)\n",
    "  y_data = np.concatenate([y1_data, y2_data], axis=0)\n",
    "  x_test = x3_data\n",
    "  x_perturbed_test = x3_perturbed_data\n",
    "  y_test = y3_data\n",
    "  # x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "  x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "  x_train_scaled,_,_ = scale_data(x_train)\n",
    "  x_val_scaled,_,_ = scale_data(x_val)\n",
    "  x_test_scaled,_,_ = scale_data(x_test)\n",
    "  x_perturbed_test_scaled,_,_ = scale_data(x_perturbed_test)\n",
    "  y_train_scaled,_,_ = scale_data(y_train)\n",
    "  y_val_scaled,_,_ = scale_data(y_val)\n",
    "  y_test_scaled,min_vals,max_vals = scale_data(y_test)\n",
    "  x_train_input = x_train_scaled#[:-10]  # shape: (N, 15, 15, 3)\n",
    "  y_train_output = y_train_scaled#[10:]  # shape: (N, 15, 15, 2)\n",
    "  x_val_input = x_val_scaled\n",
    "  y_val_output = y_val_scaled\n",
    "  x_test_input = x_test_scaled#[:-10]\n",
    "  x_perturbed_test_input = x_perturbed_test_scaled\n",
    "  y_test_output = y_test_scaled#[10:]\n",
    "  return x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_perturbed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1749570078992,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "7_x6-I7bMFhO"
   },
   "outputs": [],
   "source": [
    "def restore_original_scale(scaled_data, min_vals, max_vals):\n",
    "    # Only denormalize the first two channels (U,V components), keep sensor mask channel unchanged\n",
    "    restored_data = np.copy(scaled_data)\n",
    "    \n",
    "    if scaled_data.shape[-1] >= 3:  # If there are 3 or more channels\n",
    "        # Only denormalize the first two channels\n",
    "        restored_data[:, :, :, :2] = scaled_data[:, :, :, :2] * (max_vals - min_vals) + min_vals\n",
    "        # Sensor mask channel remains unchanged (already copied in copy)\n",
    "    else:\n",
    "        # If there are only two channels, process as before\n",
    "        restored_data = scaled_data * (max_vals - min_vals) + min_vals\n",
    "    \n",
    "    return restored_data\n",
    "\n",
    "def scale_data(data):\n",
    "    # data shape is (samples, height, width, channels)\n",
    "    # Only normalize the first two channels (U,V components), keep sensor mask channel unchanged\n",
    "    scaled_data = np.zeros_like(data, dtype=np.float32)\n",
    "    min_vals = np.zeros((data.shape[0], 1, 1, 1), dtype=np.float32)\n",
    "    max_vals = np.zeros((data.shape[0], 1, 1, 1), dtype=np.float32)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        if data.shape[-1] >= 3:  # If there are 3 or more channels\n",
    "            # Only calculate min-max for the first two channels (U,V components)\n",
    "            physics_data = data[i, :, :, :2]  # Only take the first two channels\n",
    "            min_val = np.min(physics_data)\n",
    "            max_val = np.max(physics_data)\n",
    "            min_vals[i] = min_val\n",
    "            max_vals[i] = max_val\n",
    "            \n",
    "            # Normalize the first two channels\n",
    "            scaled_data[i, :, :, :2] = (data[i, :, :, :2] - min_val) / (max_val - min_val + 1e-8)\n",
    "            \n",
    "            # Sensor mask channel remains unchanged\n",
    "            if data.shape[-1] > 2:\n",
    "                scaled_data[i, :, :, 2:] = data[i, :, :, 2:]\n",
    "        else:\n",
    "            # If there are only two channels, process as before\n",
    "            min_val = np.min(data[i])\n",
    "            max_val = np.max(data[i])\n",
    "            min_vals[i] = min_val\n",
    "            max_vals[i] = max_val\n",
    "            scaled_data[i] = (data[i] - min_val) / (max_val - min_val + 1e-8)\n",
    "    \n",
    "    return scaled_data, min_vals, max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1749570079091,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "joEpBTu_yNNo"
   },
   "outputs": [],
   "source": [
    "def visualize_results(x_data, y_data, pred_data, mask_data, sample_index=0, num_samples=8, model='', max_samples=None):\n",
    "    \"\"\"\n",
    "    Visualize U-Net results for multiple samples in a grid layout.\n",
    "\n",
    "    Parameters:\n",
    "      x_data: Input data (N, 15, 15, 3)\n",
    "      y_data: Ground truth (N, 15, 15, 2)\n",
    "      pred_data: U-Net prediction results (N, 15, 15, 2)\n",
    "      mask_data: Sensor mask data (N, 15, 15, 3)\n",
    "      sample_index: Starting sample index\n",
    "      num_samples: Number of samples to visualize (default: 8)\n",
    "    \"\"\"\n",
    "    # Ensure we don't exceed dataset size\n",
    "    # max_samples = min(num_samples, len(x_data) - sample_index)\n",
    "    # max_samples = np.random.randint(0, len(x_data), size=num_samples)\n",
    "    # Create figure with 4 rows and num_samples columns\n",
    "    fig, axs = plt.subplots(4, len(max_samples), figsize=(len(max_samples)*2.5, 10), gridspec_kw={'hspace': -0.5, 'wspace': 0.25})\n",
    "\n",
    "    # If only one sample, make axs 2D\n",
    "    if len(max_samples) == 1:\n",
    "        axs = axs.reshape(4, 1)\n",
    "\n",
    "    # Common colormap for better comparison\n",
    "    cmap = 'viridis'\n",
    "\n",
    "    # For storing min and max values for consistent color scales\n",
    "    u_min, u_max = float('inf'), float('-inf')\n",
    "    v_min, v_max = float('inf'), float('-inf')\n",
    "    error_max = 0\n",
    "\n",
    "    # Find min/max across all samples for consistent coloring\n",
    "    for i in max_samples:\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Update U min/max\n",
    "        u_min = min(u_min, np.min(x_data[idx, :, :, 0]), np.min(pred_data[idx, :, :, 0]), np.min(y_data[idx, :, :, 0]))\n",
    "        u_max = max(u_max, np.max(x_data[idx, :, :, 0]), np.max(pred_data[idx, :, :, 0]), np.max(y_data[idx, :, :, 0]))\n",
    "\n",
    "        # Update V min/max\n",
    "        v_min = min(v_min, np.min(x_data[idx, :, :, 1]), np.min(pred_data[idx, :, :, 1]), np.min(y_data[idx, :, :, 1]))\n",
    "        v_max = max(v_max, np.max(x_data[idx, :, :, 1]), np.max(pred_data[idx, :, :, 1]), np.max(y_data[idx, :, :, 1]))\n",
    "\n",
    "        # Compute errors for max scaling\n",
    "        u_error = np.abs(pred_data[idx, :, :, 0] - y_data[idx, :, :, 0])\n",
    "        v_error = np.abs(pred_data[idx, :, :, 1] - y_data[idx, :, :, 1])\n",
    "        error_max = max(error_max, np.max(u_error), np.max(v_error))\n",
    "\n",
    "    # Plot each sample\n",
    "    \n",
    "    for index, i in enumerate(max_samples):\n",
    "        idx = sample_index + i\n",
    "        if idx >= len(x_data):\n",
    "            break\n",
    "\n",
    "        # Get sensor positions\n",
    "        sensor_mask = mask_data[idx, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "\n",
    "        # Row 1: Interpolated inputs (Observation)\n",
    "        im1 = axs[0, index].imshow(x_data[idx, :, :, 0], cmap=cmap, vmin=u_min, vmax=u_max)\n",
    "        axs[0, index].set_title(f'Sample {idx}')\n",
    "        if index == 0:\n",
    "          axs[0, index].set_ylabel(f'Interpolated U', rotation=90, labelpad=15, va='center')\n",
    "        axs[0, index].scatter(sensor_positions[1], sensor_positions[0], c='red', s=10, marker='o')\n",
    "        axs[0, index].set_xticks([])\n",
    "        axs[0, index].set_yticks([])\n",
    "        fig.colorbar(im1, ax=axs[0, index], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Row 2: Predictions\n",
    "        im2 = axs[1, index].imshow(pred_data[idx, :, :, 0], cmap=cmap, vmin=u_min, vmax=u_max)\n",
    "        # axs[1, i].set_title('Predicted U')\n",
    "        if index == 0:\n",
    "          axs[1, index].set_ylabel(f'Predicted U', rotation=90, labelpad=15, va='center')\n",
    "        axs[1, index].set_xticks([])\n",
    "        axs[1, index].set_yticks([])\n",
    "        fig.colorbar(im2, ax=axs[1, index], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Row 3: Ground truth\n",
    "        im3 = axs[2, index].imshow(y_data[idx, :, :, 0], cmap=cmap, vmin=u_min, vmax=u_max)\n",
    "        if index == 0:\n",
    "          axs[2, index].set_ylabel(f'Ground Truth U', rotation=90, labelpad=15, va='center')\n",
    "        # axs[2, i].set_title('Ground Truth U')\n",
    "        axs[2, index].set_xticks([])\n",
    "        axs[2, index].set_yticks([])\n",
    "        fig.colorbar(im3, ax=axs[2, index], fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Row 4: Error\n",
    "        error = np.abs(pred_data[idx, :, :, 0] - y_data[idx, :, :, 0])\n",
    "        im4 = axs[3, index].imshow(error, cmap='hot', vmin=0, vmax=error_max)\n",
    "        if index == 0:\n",
    "          axs[3, index].set_ylabel(f'Error', rotation=90, labelpad=15, va='center')\n",
    "        #axs[3, i].set_title(f'Error (mean={error.mean():.4f})')\n",
    "        axs[3, index].set_xticks([])\n",
    "        axs[3, index].set_yticks([])\n",
    "        fig.colorbar(im4, ax=axs[3, index], fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(f'{model}', fontsize=16, fontweight='bold')\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # Also display for V component\n",
    "    # fig, axs = plt.subplots(4, max_samples, figsize=(max_samples*2.5, 10))\n",
    "\n",
    "    # # If only one sample, make axs 2D\n",
    "    # if max_samples == 1:\n",
    "    #     axs = axs.reshape(4, 1)\n",
    "\n",
    "    # # Plot each sample (V component)\n",
    "    # for i in range(max_samples):\n",
    "    #     idx = sample_index + i\n",
    "    #     if idx >= len(x_data):\n",
    "    #         break\n",
    "\n",
    "    #     # Get sensor positions\n",
    "    #     sensor_mask = mask_data[idx, :, :, 2]\n",
    "    #     sensor_positions = np.where(sensor_mask == 1)\n",
    "\n",
    "    #     # Row 1: Interpolated inputs (Observation)\n",
    "    #     im1 = axs[0, i].imshow(x_data[idx, :, :, 1], cmap=cmap, vmin=v_min, vmax=v_max)\n",
    "    #     axs[0, i].set_title(f'Sample {idx}\\nInterpolated V')\n",
    "    #     axs[0, i].scatter(sensor_positions[1], sensor_positions[0], c='red', s=10, marker='o')\n",
    "    #     axs[0, i].set_xticks([])\n",
    "    #     axs[0, i].set_yticks([])\n",
    "    #     fig.colorbar(im1, ax=axs[0, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    #     # Row 2: Predictions\n",
    "    #     im2 = axs[1, i].imshow(pred_data[idx, :, :, 1], cmap=cmap, vmin=v_min, vmax=v_max)\n",
    "    #     axs[1, i].set_title('Predicted V')\n",
    "    #     axs[1, i].set_xticks([])\n",
    "    #     axs[1, i].set_yticks([])\n",
    "    #     fig.colorbar(im2, ax=axs[1, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    #     # Row 3: Ground truth\n",
    "    #     im3 = axs[2, i].imshow(y_data[idx, :, :, 1], cmap=cmap, vmin=v_min, vmax=v_max)\n",
    "    #     axs[2, i].set_title('Ground Truth V')\n",
    "    #     axs[2, i].set_xticks([])\n",
    "    #     axs[2, i].set_yticks([])\n",
    "    #     fig.colorbar(im3, ax=axs[2, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    #     # Row 4: Error\n",
    "    #     error = np.abs(pred_data[idx, :, :, 1] - y_data[idx, :, :, 1])\n",
    "    #     im4 = axs[3, i].imshow(error, cmap='hot', vmin=0, vmax=error_max)\n",
    "    #     axs[3, i].set_title(f'Error (mean={error.mean():.4f})')\n",
    "    #     axs[3, i].set_xticks([])\n",
    "    #     axs[3, i].set_yticks([])\n",
    "    #     fig.colorbar(im4, ax=axs[3, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "# Calculate whole image vector field relative L2 error\n",
    "def calculate_whole_image_relative_l2_error(pred, true, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Calculate relative L2 error for the whole image:\n",
    "    ||(pred_u,pred_v) - (true_u,true_v)|| / (||true_u,true_v|| + epsilon)\n",
    "\n",
    "    Parameters:\n",
    "        pred: Predicted values with shape [..., 2]\n",
    "        true: True values with shape [..., 2]\n",
    "        epsilon: Small constant to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "        Single relative L2 error value for the whole image\n",
    "    \"\"\"\n",
    "    # Ensure we're working with the full vector data (UV)\n",
    "    assert pred.shape[-1] >= 2 and true.shape[-1] >= 2, \"Input must contain at least 2 channels\"\n",
    "\n",
    "    # Calculate vector difference squared for all pixels\n",
    "    u_diff = np.square(pred[..., 0] - true[..., 0])\n",
    "    v_diff = np.square(pred[..., 1] - true[..., 1])\n",
    "    total_diff_squared = np.sum(u_diff + v_diff)\n",
    "\n",
    "    # Calculate sum of true vector magnitudes squared for all pixels\n",
    "    true_u_squared = np.square(true[..., 0])\n",
    "    true_v_squared = np.square(true[..., 1])\n",
    "    total_true_squared = np.sum(true_u_squared + true_v_squared) + epsilon\n",
    "\n",
    "    # Calculate whole image relative error\n",
    "    return total_diff_squared / total_true_squared\n",
    "\n",
    "# Calculate sample-level whole image vector field relative L2 error\n",
    "def compute_sample_whole_image_relative_l2_errors(pred_test_restored, y_test):\n",
    "    \"\"\"Calculate and display sample-level whole image vector field relative L2 error (calculated per whole image)\"\"\"\n",
    "    whole_image_rel_errors = []\n",
    "    for i in range(len(pred_test_restored)):\n",
    "        # Calculate whole image vector field relative L2 error\n",
    "        rel_error = calculate_whole_image_relative_l2_error(\n",
    "            pred_test_restored[i],\n",
    "            y_test[i]\n",
    "        )\n",
    "\n",
    "        # Store whole image error for each sample\n",
    "        whole_image_rel_errors.append(rel_error)\n",
    "\n",
    "    # Print average whole image relative L2 error\n",
    "    print(f\"Sample-level Whole Image Relative L2 Error: {np.mean(whole_image_rel_errors):.4f}\")\n",
    "\n",
    "    # Plot whole image relative L2 error distribution\n",
    "    print(\"\\nPlotting Sample-level Whole Image Relative L2 Error Distribution...\")\n",
    "    plot_error_histogram(whole_image_rel_errors, bins=50, error_type=\"Whole Image Relative L2\", use_log=False, clip_percentile=95)\n",
    "\n",
    "    return whole_image_rel_errors\n",
    "# Calculate L2 error\n",
    "# def calculate_l2_error(pred, true, epsilon=1e-8):\n",
    "#     \"\"\"Calculate relative L2 error: |pred-true| / (|true| + epsilon)\"\"\"\n",
    "#     return np.square(pred - true) / (np.square(true) + epsilon)\n",
    "\n",
    "def calculate_l2_error(pred, true, epsilon=1e-8):\n",
    "    \"\"\"Calculate relative L2 error for vector field\"\"\"\n",
    "    # Squared norm of vector difference\n",
    "    vector_diff_squared = np.square(pred[:,0] - true[:,0]) + np.square(pred[:,1] - true[:,1])\n",
    "    # Squared norm of true vector\n",
    "    true_norm_squared = np.square(true[:,0]) + np.square(true[:,1]) + epsilon\n",
    "    return vector_diff_squared / true_norm_squared\n",
    "\n",
    "# Apply logarithmic transformation to error values\n",
    "def log_transform_errors(errors, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Apply logarithmic transformation to error values: log(error + epsilon)\n",
    "\n",
    "    Parameters:\n",
    "        errors: Error array\n",
    "        epsilon: Small constant to prevent taking log of 0\n",
    "\n",
    "    Returns:\n",
    "        Log-transformed errors\n",
    "    \"\"\"\n",
    "    return np.log10(np.array(errors) + epsilon)\n",
    "\n",
    "# Plot error histogram\n",
    "def plot_error_histogram(errors, bins=50, error_type=\"L2\", use_log=True, clip_percentile=None):\n",
    "    \"\"\"\n",
    "    Plot error histogram to show error distribution\n",
    "\n",
    "    Parameters:\n",
    "        errors: Error list (U and V channels already merged)\n",
    "        bins: Number of histogram bins\n",
    "        error_type: Name of error type (for title)\n",
    "        use_log: Whether to use logarithmic transformation\n",
    "        clip_percentile: Clipping percentile, e.g., 95 means only keep errors less than 95th percentile\n",
    "    \"\"\"\n",
    "    # Process errors\n",
    "    if clip_percentile is not None:\n",
    "        error_max = np.percentile(errors, clip_percentile)\n",
    "        errors_clipped = [e for e in errors if e <= error_max]\n",
    "    else:\n",
    "        errors_clipped = errors\n",
    "\n",
    "    # Logarithmic transformation\n",
    "    if use_log:\n",
    "        errors_processed = log_transform_errors(errors_clipped)\n",
    "        transform_label = \"Log\"\n",
    "    else:\n",
    "        errors_processed = errors_clipped\n",
    "        transform_label = \"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot error histogram\n",
    "    plt.hist(errors_processed, bins=bins, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(np.mean(errors_processed), color='red', linestyle='dashed', linewidth=1, label=f'Mean: {np.mean(errors_processed):.4f}')\n",
    "    plt.axvline(np.median(errors_processed), color='green', linestyle='dashed', linewidth=1, label=f'Median: {np.median(errors_processed):.4f}')\n",
    "    plt.title(f'Sample-level Combined U+V {error_type} {transform_label} Error')\n",
    "    plt.xlabel(f'Error Value{\" (Log Scale)\" if use_log else \"\"}')\n",
    "    plt.ylabel('Sample Count')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    print(np.max(errors_processed))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # Cumulative distribution plot\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # # Cumulative distribution\n",
    "    # plt.hist(errors_processed, bins=bins, alpha=0.7, color='blue', edgecolor='black', cumulative=True, density=True)\n",
    "    # plt.title(f'Sample-level Combined U+V {error_type} {transform_label} Error CDF')\n",
    "    # plt.xlabel(f'Error Value{\" (Log Scale)\" if use_log else \"\"}')\n",
    "    # plt.ylabel('Cumulative Probability')\n",
    "    # plt.grid(alpha=0.3)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1749570079774,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "t3eCggUN5JKd"
   },
   "outputs": [],
   "source": [
    "# Add additional evaluation metric functions\n",
    "def calculate_fractional_bias(exp, pred, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate Fractional Bias (FB)\n",
    "    FB = (mean experimental value - mean predicted value) / (0.5 * (mean experimental value + mean predicted value))\n",
    "\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        epsilon: Small constant to prevent division by zero\n",
    "    Returns:\n",
    "        FB value, typically in range [-2, 2], 0 indicates no bias\n",
    "    \"\"\"\n",
    "    mean_exp = np.mean(exp)\n",
    "    mean_pred = np.mean(pred)\n",
    "    denominator = 0.5 * (mean_exp + mean_pred)\n",
    "    if abs(denominator) < epsilon:\n",
    "        return 0.0\n",
    "    return (mean_exp - mean_pred) / denominator\n",
    "\n",
    "def calculate_geometric_mean_bias(exp, pred, epsilon=1e-10, max_value=1000.0):\n",
    "    \"\"\"\n",
    "    Calculate Geometric Mean Bias (MG)\n",
    "    MG = exp(mean(ln(experimental value) - ln(predicted value)))\n",
    "\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        epsilon: Small constant to prevent ln(0)\n",
    "        max_value: Upper limit of return value to prevent extreme values\n",
    "    Returns:\n",
    "        MG value, typically in range [0, max_value], 1 indicates no bias\n",
    "    \"\"\"\n",
    "    # Prevent taking logarithm of negative values or zero\n",
    "    valid_indices = (exp > epsilon) & (pred > epsilon)\n",
    "    if np.sum(valid_indices) == 0:\n",
    "        return 1.0  # If there is no valid data, return 1 to indicate no bias\n",
    "\n",
    "    exp_safe = exp[valid_indices]\n",
    "    pred_safe = pred[valid_indices]\n",
    "\n",
    "    ln_diff = np.mean(np.log(exp_safe / pred_safe))\n",
    "    # Prevent extreme values\n",
    "    ln_diff = np.clip(ln_diff, -np.log(max_value), np.log(max_value))\n",
    "    return np.exp(ln_diff)\n",
    "\n",
    "def calculate_normalized_mean_square_error(exp, pred, epsilon=1e-10, max_value=1000.0):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Mean Square Error (NMSE)\n",
    "    NMSE = mean((experimental value - predicted value)^2) / (mean experimental value * mean predicted value)\n",
    "\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        epsilon: Small constant to prevent division by zero\n",
    "        max_value: Upper limit of return value to prevent extreme values\n",
    "    Returns:\n",
    "        NMSE value, typically in range [0, max_value], 0 indicates perfect prediction\n",
    "    \"\"\"\n",
    "    # Calculate mean square error\n",
    "    mse = np.mean(np.square(exp - pred))\n",
    "\n",
    "    # Calculate denominator\n",
    "    mean_exp = np.abs(np.mean(exp)) + epsilon\n",
    "    mean_pred = np.abs(np.mean(pred)) + epsilon\n",
    "    denominator = mean_exp * mean_pred\n",
    "\n",
    "    # Ensure result is positive\n",
    "    result = np.abs(mse / denominator)\n",
    "\n",
    "    # Limit maximum value\n",
    "    return min(result, max_value)\n",
    "\n",
    "def calculate_geometric_variance(exp, pred, epsilon=1e-10, max_value=1000.0):\n",
    "    \"\"\"\n",
    "    Calculate Geometric Variance (VG)\n",
    "    VG = exp(mean((ln(experimental value) - ln(predicted value))^2))\n",
    "\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        epsilon: Small constant to prevent ln(0)\n",
    "        max_value: Upper limit of return value to prevent overflow\n",
    "    Returns:\n",
    "        VG value, typically in range [1, max_value], 1 indicates perfect prediction\n",
    "    \"\"\"\n",
    "    # Prevent taking logarithm of negative values or zero\n",
    "    # Filter out non-positive point pairs\n",
    "    valid_indices = (exp > epsilon) & (pred > epsilon)\n",
    "    if np.sum(valid_indices) == 0:\n",
    "        return 1.0  # If there is no valid data, return 1 to indicate no bias\n",
    "\n",
    "    exp_safe = exp[valid_indices]\n",
    "    pred_safe = pred[valid_indices]\n",
    "\n",
    "    # Calculate squared logarithmic difference\n",
    "    ln_diff_squared = np.square(np.log(exp_safe) - np.log(pred_safe))\n",
    "\n",
    "    # Prevent extreme values\n",
    "    mean_ln_diff_squared = np.mean(ln_diff_squared)\n",
    "    # Limit maximum exponent value to prevent exp overflow\n",
    "    mean_ln_diff_squared = min(mean_ln_diff_squared, np.log(max_value))\n",
    "\n",
    "    return np.exp(mean_ln_diff_squared)\n",
    "\n",
    "def calculate_fac2(exp, pred, W=0.005, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate FAC2 evaluation metric\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        W: Allowed error range\n",
    "    Returns:\n",
    "        FAC2 value\n",
    "    \"\"\"\n",
    "    # Flatten input to 1D array\n",
    "    exp_flat = exp.flatten()\n",
    "    pred_flat = pred.flatten()\n",
    "\n",
    "    # Calculate number of data points satisfying FAC2 condition\n",
    "    n = len(exp_flat)\n",
    "    count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        Pi = pred_flat[i]\n",
    "        Oi = exp_flat[i]\n",
    "\n",
    "        # If Oi is zero, replace with epsilon\n",
    "        if Oi == 0:\n",
    "            Oi = epsilon\n",
    "\n",
    "        if 0.5 <= Pi / Oi <= 2:\n",
    "            count += 1\n",
    "        elif abs(Oi) <= W and abs(Pi) <= W:\n",
    "            count += 1\n",
    "\n",
    "    # Calculate FAC2\n",
    "    FAC2 = count / n\n",
    "    return FAC2\n",
    "\n",
    "def calculate_all_metrics(exp, pred, max_value=1000.0, W=0.005):\n",
    "    \"\"\"\n",
    "    Calculate all evaluation metrics\n",
    "\n",
    "    Parameters:\n",
    "        exp: Experimental/true values\n",
    "        pred: Model predicted values\n",
    "        max_value: Upper limit of return value to prevent extreme values\n",
    "    Returns:\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Flatten input to 1D array\n",
    "    exp_flat = exp.flatten()\n",
    "    pred_flat = pred.flatten()\n",
    "\n",
    "    # Print some statistics to help debugging\n",
    "    # print(f\"Debug info - True values: min={np.min(exp_flat):.4f}, max={np.max(exp_flat):.4f}, mean={np.mean(exp_flat):.4f}\")\n",
    "    # print(f\"Debug info - Predicted values: min={np.min(pred_flat):.4f}, max={np.max(pred_flat):.4f}, mean={np.mean(pred_flat):.4f}\")\n",
    "\n",
    "    # Calculate each metric\n",
    "    metrics = {\n",
    "        # 'FB': calculate_fractional_bias(exp_flat, pred_flat),\n",
    "        'MG': calculate_geometric_mean_bias(exp_flat, pred_flat, max_value=max_value),\n",
    "        'NMSE': calculate_normalized_mean_square_error(exp_flat, pred_flat, max_value=max_value),\n",
    "        # 'VG': calculate_geometric_variance(exp_flat, pred_flat, max_value=max_value),\n",
    "        'FAC2': calculate_fac2(exp, pred, W=W)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1749570079776,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "cR6ucY2G_nVO"
   },
   "outputs": [],
   "source": [
    "def apply_kriging_reconstruction(x_test, y_test, method='ordinary'):\n",
    "    \"\"\"\n",
    "    Reconstruct flow field using Kriging method, use unified optimal correlation length for entire dataset\n",
    "\n",
    "    Parameters:\n",
    "        x_test: Input data, containing interpolation results and sensor mask (N, 15, 15, 3)\n",
    "        y_test: True data for evaluation (N, 15, 15, 2)\n",
    "        method: 'ordinary' or 'universal', default is 'ordinary'\n",
    "\n",
    "    Returns:\n",
    "        pred_kriging: Kriging reconstruction results (N, 15, 15, 2)\n",
    "    \"\"\"\n",
    "    print(f\"Reconstructing flow field using {method}Kriging method...\")\n",
    "\n",
    "    # Get grid dimensions\n",
    "    h, w = x_test.shape[1:3]\n",
    "    grid_x = np.arange(0, w, 1.0)\n",
    "    grid_y = np.arange(0, h, 1.0)\n",
    "\n",
    "    # Define correlation length range to test\n",
    "    # correlation_lengths = np.linspace(2, 25.0, 20)  # From 0.5 to 10, 20 values total\n",
    "\n",
    "    # # First find optimal correlation length\n",
    "    # print(\"Finding optimal correlation length...\")\n",
    "    # best_length_u = None\n",
    "    # best_length_v = None\n",
    "    # best_error_u = float('inf')\n",
    "    # best_error_v = float('inf')\n",
    "\n",
    "    # # Use partial samples for correlation length optimization (to save time)\n",
    "    # sample_indices = np.random.choice(len(x_test), min(5000, len(x_test)), replace=False)\n",
    "\n",
    "    # for length in tqdm(correlation_lengths, desc=\"Optimizing correlation length\"):\n",
    "    #     total_error_u = 0\n",
    "    #     total_error_v = 0\n",
    "    #     valid_samples = 0\n",
    "\n",
    "    #     for i in sample_indices:\n",
    "    #         sensor_mask = x_test[i, :, :, 2]\n",
    "    #         sensor_positions = np.where(sensor_mask == 1)\n",
    "    #         x_coords = sensor_positions[1]\n",
    "    #         y_coords = sensor_positions[0]\n",
    "\n",
    "    #         if len(x_coords) < 3:\n",
    "    #             continue\n",
    "\n",
    "    #         for c in range(2):\n",
    "    #             values = x_test[i, y_coords, x_coords, c]\n",
    "\n",
    "    #             try:\n",
    "    #                 if method.lower() == 'ordinary':\n",
    "    #                     krig = OrdinaryKriging(\n",
    "    #                         x_coords, y_coords, values,\n",
    "    #                         variogram_model='gaussian',\n",
    "    #                         variogram_parameters={'range': length, 'sill': 1.0, 'nugget': 0.1},\n",
    "    #                         verbose=False,\n",
    "    #                         enable_plotting=False\n",
    "    #                     )\n",
    "    #                 else:\n",
    "    #                     krig = UniversalKriging(\n",
    "    #                         x_coords, y_coords, values,\n",
    "    #                         variogram_model='gaussian',\n",
    "    #                         variogram_parameters={'range': length, 'sill': 1.0, 'nugget': 0.1},\n",
    "    #                         verbose=False,\n",
    "    #                         enable_plotting=False\n",
    "    #                     )\n",
    "\n",
    "    #                 z, ss = krig.execute('grid', grid_x, grid_y)\n",
    "    #                 pred_values = z[y_coords, x_coords]\n",
    "    #                 error = np.mean((pred_values - values)**2)\n",
    "\n",
    "    #                 if c == 0:\n",
    "    #                     total_error_u += error\n",
    "    #                 else:\n",
    "    #                     total_error_v += error\n",
    "\n",
    "    #             except Exception as e:\n",
    "    #                 continue\n",
    "\n",
    "    #         valid_samples += 1\n",
    "\n",
    "    #     if valid_samples > 0:\n",
    "    #         avg_error_u = total_error_u / valid_samples\n",
    "    #         avg_error_v = total_error_v / valid_samples\n",
    "\n",
    "    #         if avg_error_u < best_error_u:\n",
    "    #             best_error_u = avg_error_u\n",
    "    #             best_length_u = length\n",
    "    #         if avg_error_v < best_error_v:\n",
    "    #             best_error_v = avg_error_v\n",
    "    #             best_length_v = length\n",
    "\n",
    "    # print(f\"Optimal correlation length - U component: {best_length_u:.3f}, V component: {best_length_v:.3f}\")\n",
    "    best_length_u = 10\n",
    "    best_length_v = 10\n",
    "    # Predict all samples using optimal correlation length\n",
    "    print(\"Predicting using optimal correlation length...\")\n",
    "    pred_kriging = np.zeros_like(y_test)\n",
    "\n",
    "    # for i in tqdm(range(len(x_test)), desc=\"Kriging reconstruction progress\"):\n",
    "    for i in range(len(x_test)):\n",
    "        sensor_mask = x_test[i, :, :, 2]\n",
    "        sensor_positions = np.where(sensor_mask == 1)\n",
    "        x_coords = sensor_positions[1]\n",
    "        y_coords = sensor_positions[0]\n",
    "\n",
    "        for c in range(2):\n",
    "            values = x_test[i, y_coords, x_coords, c]\n",
    "\n",
    "            if len(x_coords) < 3:\n",
    "                pred_kriging[i, :, :, c] = x_test[i, :, :, c]\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if method.lower() == 'ordinary':\n",
    "                    krig = OrdinaryKriging(\n",
    "                        x_coords, y_coords, values,\n",
    "                        variogram_model='gaussian',\n",
    "                        variogram_parameters={'range': best_length_u if c == 0 else best_length_v,\n",
    "                                            'sill': 1.0, 'nugget': 0},\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "                else:\n",
    "                    krig = UniversalKriging(\n",
    "                        x_coords, y_coords, values,\n",
    "                        variogram_model='gaussian',\n",
    "                        variogram_parameters={'range': best_length_u if c == 0 else best_length_v,\n",
    "                                            'sill': 1.0, 'nugget': 0},\n",
    "                        verbose=False,\n",
    "                        enable_plotting=False\n",
    "                    )\n",
    "\n",
    "                z, ss = krig.execute('grid', grid_x, grid_y)\n",
    "                pred_kriging[i, :, :, c] = z\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Kriging interpolation failed for sample {i}, component {c}: {e}\")\n",
    "                pred_kriging[i, :, :, c] = x_test[i, :, :, c]\n",
    "\n",
    "    return pred_kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou1QOLO_fLie"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osM-nLOQVWEO"
   },
   "source": [
    "### CWGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdyVWzlnVaOP"
   },
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h5N57R-VdSB"
   },
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"Wasserstein loss function\"\"\"\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "@register_keras_serializable()\n",
    "def gradient_penalty(discriminator, condition_data, real_samples, fake_samples, batch_size):\n",
    "    \"\"\"Gradient penalty term\"\"\"\n",
    "    # Random interpolation\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0., 1.)\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator([condition_data, interpolated], training=True)\n",
    "\n",
    "    gradients = tape.gradient(pred, interpolated)\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "    gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "def build_cwgan_generator(input_shape, noise_dim=100):\n",
    "    \"\"\"Build conditional WGAN generator\"\"\"\n",
    "    # Conditional input (sensor data)\n",
    "    condition_input = Input(shape=input_shape, name='condition_input')\n",
    "\n",
    "    # Noise input\n",
    "    noise_input = Input(shape=(noise_dim,), name='noise_input')\n",
    "\n",
    "    # Reshape noise to same spatial dimensions as conditional input\n",
    "    noise_reshaped = Dense(input_shape[0] * input_shape[1] * 1)(noise_input)\n",
    "    noise_reshaped = Reshape((input_shape[0], input_shape[1], 1))(noise_reshaped)\n",
    "\n",
    "    # Merge conditional input and noise\n",
    "    merged = concatenate([condition_input, noise_reshaped])\n",
    "\n",
    "    # === Step 1: ZeroPadding to 16x16 ===\n",
    "    padded_inputs = ZeroPadding2D(((0, 1), (0, 1)))(merged)  # 15x15 -> 16x16\n",
    "\n",
    "    # === Encoder part ===\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(padded_inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2), padding=\"valid\")(c1)  # 16x16 -> 8x8\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2), padding=\"valid\")(c2)  # 8x8 -> 4x4\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # === Decoder part ===\n",
    "    u4 = UpSampling2D((2, 2))(c3)  # 4x4 -> 8x8\n",
    "    u4 = concatenate([u4, c2])  # 8x8 connected with c2\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)  # 8x8 -> 16x16\n",
    "    u5 = concatenate([u5, c1])  # 16x16 connected with c1\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # === Step 2: Cropping2D remove padding, restore to 15x15 ===\n",
    "    cropped_outputs = Cropping2D(((0, 1), (0, 1)))(c5)  # 16x16 -> 15x15\n",
    "\n",
    "    outputs = Conv2D(2, (1, 1), activation='tanh')(cropped_outputs)\n",
    "\n",
    "    model = Model([condition_input, noise_input], outputs, name='generator')\n",
    "    return model\n",
    "\n",
    "def build_cwgan_discriminator(input_shape):\n",
    "    \"\"\"Build conditional WGAN discriminator\"\"\"\n",
    "    # Conditional input (sensor data)\n",
    "    condition_input = Input(shape=input_shape, name='condition_input')\n",
    "\n",
    "    # Real/generated data input\n",
    "    data_input = Input(shape=(input_shape[0], input_shape[1], 2), name='data_input')\n",
    "\n",
    "    # Merge condition and data\n",
    "    merged = concatenate([condition_input, data_input])\n",
    "\n",
    "    # === Step 1: ZeroPadding to 16x16 ===\n",
    "    padded_inputs = ZeroPadding2D(((0, 1), (0, 1)))(merged)  # 15x15 -> 16x16\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(64, (4, 4), strides=2, padding='same')(padded_inputs)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(512, (4, 4), strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Output layer (no sigmoid, because it's Wasserstein loss)\n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    model = Model([condition_input, data_input], outputs, name='discriminator')\n",
    "    return model\n",
    "\n",
    "class CWGAN(tf.keras.Model):\n",
    "    \"\"\"Conditional Wasserstein GAN model\"\"\"\n",
    "\n",
    "    def __init__(self, generator, discriminator, noise_dim=100, gp_weight=10.0):\n",
    "        super(CWGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.noise_dim = noise_dim\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer):\n",
    "        super(CWGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        condition_data, real_data = data\n",
    "        batch_size = tf.shape(condition_data)[0]\n",
    "\n",
    "        # Train discriminator\n",
    "        for _ in range(5):  # Discriminator trains 5 times, generator trains once\n",
    "            noise = tf.random.normal([batch_size, self.noise_dim])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake data\n",
    "                fake_data = self.generator([condition_data, noise], training=True)\n",
    "\n",
    "                # Discriminator prediction\n",
    "                real_pred = self.discriminator([condition_data, real_data], training=True)\n",
    "                fake_pred = self.discriminator([condition_data, fake_data], training=True)\n",
    "\n",
    "                # Wasserstein loss\n",
    "                d_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
    "\n",
    "                # Gradient penalty\n",
    "                gp = gradient_penalty(self.discriminator, condition_data, real_data, fake_data, batch_size)\n",
    "                d_loss += self.gp_weight * gp\n",
    "\n",
    "            # Update discriminator\n",
    "            d_gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train generator\n",
    "        noise = tf.random.normal([batch_size, self.noise_dim])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_data = self.generator([condition_data, noise], training=True)\n",
    "            fake_pred = self.discriminator([condition_data, fake_data], training=True)\n",
    "\n",
    "            # Generator loss (hope discriminator thinks generated data is real)\n",
    "            g_loss = -tf.reduce_mean(fake_pred)\n",
    "\n",
    "            # Add L1 loss to improve reconstruction quality\n",
    "            l1_loss = tf.reduce_mean(tf.abs(real_data - fake_data))\n",
    "            g_loss += 100.0 * l1_loss  # L1 loss weight\n",
    "\n",
    "        # Update generator\n",
    "        g_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"l1_loss\": l1_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAZFbybCVpGd"
   },
   "source": [
    "#### For loop train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2BU7qX_VYYM"
   },
   "outputs": [],
   "source": [
    "sensor_list = [5,10,15,20,25,30]\n",
    "# sensor_list = [30]\n",
    "from tqdm.notebook import tqdm\n",
    "for s in range(len(sensor_list)):\n",
    "    sensor_num = sensor_list[s]\n",
    "    x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test = load_my_data(sensor_num, method=method)\n",
    "    print(f'Dataset for {sensor_num} is ready.')\n",
    "\n",
    "    input_shape = (15, 15, 3)\n",
    "    noise_dim = 100\n",
    "\n",
    "    # Build model\n",
    "    generator = build_cwgan_generator(input_shape, noise_dim)\n",
    "    discriminator = build_cwgan_discriminator(input_shape)\n",
    "\n",
    "    # Create CWGAN\n",
    "    cwgan = CWGAN(generator, discriminator, noise_dim)\n",
    "\n",
    "    # Compile model\n",
    "    g_optimizer = optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "    d_optimizer = optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "    cwgan.compile(g_optimizer, d_optimizer)\n",
    "\n",
    "    save_dir = model_dir\n",
    "\n",
    "    # # Train\n",
    "    # # Training parameters\n",
    "    # batch_size = 8\n",
    "    # epochs = 100\n",
    "\n",
    "    # # Create dataset\n",
    "    # train_dataset = tf.data.Dataset.from_tensor_slices((x_train_input, y_train_output))\n",
    "    # train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "    # # Train model\n",
    "    # print(f\"Starting CWGAN training, sensor count: {sensor_num}\")\n",
    "    # # print(f\"Total {epochs} epochs, approximately {len(x_train_input)//batch_size} batches per epoch\")\n",
    "    # # Early stopping parameters\n",
    "    # best_val_loss = float('inf')\n",
    "    # patience = 10  # Stop if no improvement for 15 epochs\n",
    "    # patience_counter = 0\n",
    "    # best_epoch = 0\n",
    "\n",
    "    # # Create validation dataset\n",
    "    # val_dataset = tf.data.Dataset.from_tensor_slices((x_val_input, y_val_output))\n",
    "    # val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     epoch_d_loss = 0\n",
    "    #     epoch_g_loss = 0\n",
    "    #     epoch_l1_loss = 0\n",
    "    #     num_batches = 0\n",
    "\n",
    "    #     # Use tqdm to show batch progress\n",
    "    #     batch_iterator = tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "    #     for batch in batch_iterator:\n",
    "    #         losses = cwgan.train_step(batch)\n",
    "    #         epoch_d_loss += losses[\"d_loss\"]\n",
    "    #         epoch_g_loss += losses[\"g_loss\"]\n",
    "    #         epoch_l1_loss += losses[\"l1_loss\"]\n",
    "    #         num_batches += 1\n",
    "\n",
    "    #         # Update progress bar to show current loss\n",
    "    #         batch_iterator.set_postfix({\n",
    "    #             'D_loss': f'{losses[\"d_loss\"]:.4f}',\n",
    "    #             'G_loss': f'{losses[\"g_loss\"]:.4f}',\n",
    "    #             'L1_loss': f'{losses[\"l1_loss\"]:.4f}'\n",
    "    #         })\n",
    "\n",
    "    #     # Display average loss after each epoch\n",
    "    #     avg_d_loss = epoch_d_loss/num_batches\n",
    "    #     avg_g_loss = epoch_g_loss/num_batches\n",
    "    #     avg_l1_loss = epoch_l1_loss/num_batches\n",
    "\n",
    "    #     # Validation set evaluation and early stopping logic\n",
    "    #     val_g_loss = 0\n",
    "    #     val_l1_loss = 0\n",
    "    #     val_batches = 0\n",
    "\n",
    "    #     for val_batch in val_dataset:\n",
    "    #         val_condition_data, val_real_data = val_batch\n",
    "    #         val_noise = tf.random.normal([tf.shape(val_condition_data)[0], noise_dim])\n",
    "    #         val_fake_data = generator([val_condition_data, val_noise], training=False)\n",
    "\n",
    "    #         val_fake_pred = discriminator([val_condition_data, val_fake_data], training=False)\n",
    "    #         val_g_loss_batch = -tf.reduce_mean(val_fake_pred)\n",
    "    #         val_l1_loss_batch = tf.reduce_mean(tf.abs(val_real_data - val_fake_data))\n",
    "\n",
    "    #         val_g_loss += val_g_loss_batch\n",
    "    #         val_l1_loss += val_l1_loss_batch\n",
    "    #         val_batches += 1\n",
    "\n",
    "    #     avg_val_g_loss = val_g_loss / val_batches\n",
    "    #     avg_val_l1_loss = val_l1_loss / val_batches\n",
    "    #     val_total_loss = avg_val_g_loss + 100.0 * avg_val_l1_loss\n",
    "\n",
    "    #     print(f\"Epoch {epoch+1}/{epochs} - D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}, L1 Loss: {avg_l1_loss:.4f}\")\n",
    "    #     print(f\"Validation - G Loss: {avg_val_g_loss:.4f}, L1 Loss: {avg_val_l1_loss:.4f}, Total: {val_total_loss:.4f}\")\n",
    "\n",
    "    #     # Early stopping check\n",
    "    #     if np.abs(val_total_loss) < np.abs(best_val_loss):\n",
    "    #         best_val_loss = val_total_loss\n",
    "    #         best_epoch = epoch + 1\n",
    "    #         patience_counter = 0\n",
    "    #         generator.save(f'{model_dir}{method}_CWGAN-Generator_{sensor_num}_best.keras')\n",
    "    #         print(f\" New best model! Validation loss: {best_val_loss:.4f}\")\n",
    "    #     else:\n",
    "    #         patience_counter += 1\n",
    "    #         print(f\" Validation loss not improving ({patience_counter}/{patience})\")\n",
    "\n",
    "    #         if patience_counter >= patience:\n",
    "    #             print(f\" Early stopping! Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}\")\n",
    "    #             break\n",
    "\n",
    "    #     # Save checkpoint every 10 epochs\n",
    "    #     if (epoch + 1) % 10 == 0:\n",
    "    #         print(f\"Saving checkpoint - Epoch {epoch+1}\")\n",
    "    #         generator.save(f'{model_dir}{method}_CWGAN-Generator_{sensor_num}_epoch_{epoch+1}.keras')\n",
    "\n",
    "    # # Save generator\n",
    "    # generator.save(f'{model_dir}{method}_CWGAN-Generator_{sensor_num}.keras')\n",
    "    # print(f\" Training completed! Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Load\n",
    "    generator_path = f'{model_dir}{method}_CWGAN-Generator_{sensor_num}_best.keras'\n",
    "    print(f\"Loading best model: {generator_path}\")\n",
    "    generator = load_model(generator_path)\n",
    "    num_generation = 1\n",
    "\n",
    "    for t in range(2):\n",
    "        if t == 0:\n",
    "            # Predict test set\n",
    "            # num_generation = 10\n",
    "            pred_tests = []\n",
    "            for _ in range(num_generation):\n",
    "                test_noise = tf.random.normal([len(x_test_input), noise_dim])\n",
    "                start_time = time.time()\n",
    "                pred_single = generator.predict([x_test_input, test_noise])\n",
    "                end_time = time.time()\n",
    "                print(f\"inference time: {end_time - start_time:.2f} seconds\")\n",
    "                pred_tests.append(pred_single)\n",
    "            pred_test = np.mean(pred_tests, axis=0)\n",
    "            # test_noise = tf.random.normal([len(x_test_input), noise_dim])\n",
    "            # pred_test = generator.predict([x_test_input, test_noise])\n",
    "        elif t == 1:\n",
    "            # Predict perturbed test set\n",
    "            # num_generation = 30\n",
    "            pred_tests = []\n",
    "            for _ in range(num_generation):\n",
    "                test_noise = tf.random.normal([len(x_perturbed_test_input), noise_dim])\n",
    "                pred_single = generator.predict([x_perturbed_test_input, test_noise])\n",
    "                pred_tests.append(pred_single)\n",
    "            pred_test = np.mean(pred_tests, axis=0)\n",
    "            # test_noise = tf.random.normal([len(x_perturbed_test_input), noise_dim])\n",
    "            # pred_test = generator.predict([x_perturbed_test_input, test_noise])\n",
    "\n",
    "        # # Denormalize (restore original data scale)\n",
    "        # pred_test_restored = restore_original_scale(pred_test, min_vals, max_vals)\n",
    "\n",
    "        # # Calculate SSIM and PSNR scores\n",
    "        # ssim_scores = []\n",
    "        # # psnr_scores = []\n",
    "        # for i in range(len(pred_test_restored)):\n",
    "        #     ssim_u = ssim(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(), multichannel=False)\n",
    "        #     ssim_v = ssim(pred_test_restored[i, :, :, 1], y_test[i, :, :, 1], data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(), multichannel=False)\n",
    "        #     ssim_score = (ssim_u + ssim_v) / 2\n",
    "        #     # psnr_score = psnr(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0],\n",
    "        #     #                 data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "        #     ssim_scores.append(ssim_score)\n",
    "        #     # psnr_scores.append(psnr_score)\n",
    "\n",
    "        # # Calculate average SSIM and PSNR\n",
    "        # average_ssim = np.mean(ssim_scores)\n",
    "        # # average_psnr = np.mean(psnr_scores)\n",
    "\n",
    "        # cwgan_metrics = []\n",
    "        # for i in range(len(pred_test_restored)):\n",
    "        #     # Calculate for U and V components separately\n",
    "        #     u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_test_restored[i, :, :, 0])\n",
    "        #     v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_test_restored[i, :, :, 1])\n",
    "\n",
    "        #     # Combine U and V metrics (take average)\n",
    "        #     combined_metrics = {}\n",
    "        #     for key in u_metrics:\n",
    "        #         combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "        #     cwgan_metrics.append(combined_metrics)\n",
    "\n",
    "        # # Calculate average metrics\n",
    "        # cwgan_avg_metrics = {}\n",
    "        # for key in cwgan_metrics[0]:\n",
    "        #     cwgan_avg_metrics[key] = np.mean([m[key] for m in cwgan_metrics])\n",
    "\n",
    "        # cwgan_avg_metrics['SSIM'] = average_ssim\n",
    "        # # cwgan_avg_metrics['PSNR'] = average_psnr\n",
    "\n",
    "        # print(\"\\n=== CWGAN Evaluation Metrics ===\")\n",
    "        # for key, value in cwgan_avg_metrics.items():\n",
    "        #     print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "        # if t == 0:\n",
    "        #     np.save(f'{metric_dir}{method}_avg30_cwgan_metrics_{sensor_num}.npy', cwgan_avg_metrics)\n",
    "        #     np.save(f'{pred_dir}{method}_avg30_cwgan_base_{sensor_num}_y_predict.npy', pred_test_restored)\n",
    "        #     print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "        # elif t == 1:\n",
    "        #     np.save(f'{metric_dir}{method}_avg30_cwgan_perturbed_metrics_{sensor_num}.npy', cwgan_avg_metrics)\n",
    "        #     np.save(f'{pred_dir}{method}_avg30_cwgan_base_{sensor_num}_y_perturbed_predict.npy', pred_test_restored)\n",
    "        #     print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSgcvw3z-9ca"
   },
   "source": [
    "### UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzugz-87CH7Y"
   },
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTsCO9nQCKBo"
   },
   "outputs": [],
   "source": [
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # === Step 1: ZeroPadding to 16x16 ===\n",
    "    padded_inputs = ZeroPadding2D(((0, 1), (0, 1)))(inputs)  # 15x15 -> 16x16\n",
    "\n",
    "    # === Encoder part ===\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(padded_inputs)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2), padding=\"valid\")(c1)  # 16x16 -> 8x8\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2), padding=\"valid\")(c2)  # 8x8 -> 4x4\n",
    "\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # === Decoder part ===\n",
    "    u4 = UpSampling2D((2, 2))(c3)  # 4x4 -> 8x8\n",
    "    u4 = concatenate([u4, c2])  # 8x8 concatenate with c2\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)  # 8x8 -> 16x16\n",
    "    u5 = concatenate([u5, c1])  # 16x16 concatenate with c1\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # === Step 2: Cropping2D remove padding, restore to 15x15 ===\n",
    "    cropped_outputs = Cropping2D(((0, 1), (0, 1)))(c5)  # 16x16 -> 15x15\n",
    "\n",
    "    outputs = Conv2D(2, (1, 1), activation='linear')(cropped_outputs)\n",
    "    model = Model(inputs, outputs)\n",
    "    # model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.compile(optimizer='adam', loss=weighted_vector_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc5oaSzHB6jS"
   },
   "source": [
    "#### For loop train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8q1fDcjB9hO"
   },
   "outputs": [],
   "source": [
    "sensor_list = [5, 10, 15, 20, 25, 30]\n",
    "# sensor_list = [30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test = load_my_data(sensor_num, method=method)\n",
    "  print(f'Dataset for {sensor_num} is ready.')\n",
    "  input_shape = (15, 15, 3)\n",
    "  unet_model = build_unet(input_shape)\n",
    "  save_dir = model_dir\n",
    "  # # Train\n",
    "  # # Training parameters\n",
    "  # batch_size = 8\n",
    "  # epochs = 100\n",
    "\n",
    "  # # Set callback functions (save best model)\n",
    "  # model_checkpoint = ModelCheckpoint(\n",
    "  #     filepath=f'{save_dir}{method}_U-Net-ImageReconstruction_{sensor_num}.keras',\n",
    "  #     monitor='val_loss', save_best_only=True, verbose=1, mode='min'\n",
    "  # )\n",
    "  # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5)\n",
    "  # early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "  # # Train model\n",
    "  # history = unet_model.fit(\n",
    "  #     x_train_input, y_train_output,\n",
    "  #     epochs=epochs,\n",
    "  #     batch_size=batch_size,\n",
    "  #     validation_data=(x_val_input, y_val_output),\n",
    "  #     callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
    "  #     shuffle=True\n",
    "  # )\n",
    "\n",
    "  # Load\n",
    "  model_path = f'{save_dir}{method}_U-Net-ImageReconstruction_{sensor_num}.keras'\n",
    "  unet_model = load_model(model_path)\n",
    "\n",
    "  for t in range(2):\n",
    "    if t == 0:\n",
    "      # Predict test set\n",
    "      start_time = time.time()\n",
    "      pred_test = unet_model.predict(x_test_input)\n",
    "      end_time = time.time()\n",
    "      print(f\"inference time: {end_time - start_time:.2f} seconds\")\n",
    "    elif t == 1:\n",
    "      pred_test = unet_model.predict(x_perturbed_test_input)\n",
    "\n",
    "\n",
    "    # Denormalize (restore original data scale)\n",
    "    # pred_test_restored = restore_original_scale(pred_test, min_vals, max_vals)\n",
    "\n",
    "    # # Calculate SSIM and PSNR scores\n",
    "    # ssim_scores = []\n",
    "    # # psnr_scores = []\n",
    "    # for i in range(len(pred_test_restored)):\n",
    "    #     # Calculate SSIM for U and V components separately\n",
    "    #     ssim_u = ssim(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(), multichannel=False)\n",
    "    #     ssim_v = ssim(pred_test_restored[i, :, :, 1], y_test[i, :, :, 1], data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(), multichannel=False)\n",
    "    #     ssim_score = (ssim_u + ssim_v) / 2\n",
    "    #     # psnr_score = psnr(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "    #     ssim_scores.append(ssim_score)\n",
    "    #     # psnr_scores.append(psnr_score)\n",
    "\n",
    "    # # Calculate average SSIM and PSNR\n",
    "    # average_ssim = np.mean(ssim_scores)\n",
    "    # # average_psnr = np.mean(psnr_scores)\n",
    "\n",
    "    # unet_metrics = []\n",
    "    # for i in range(len(pred_test_restored)):\n",
    "    #     # Calculate for U and V components separately\n",
    "    #     u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_test_restored[i, :, :, 0])\n",
    "    #     v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_test_restored[i, :, :, 1])\n",
    "\n",
    "    #     # Combine U and V metrics (take average)\n",
    "    #     combined_metrics = {}\n",
    "    #     for key in u_metrics:\n",
    "    #         combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "    #     unet_metrics.append(combined_metrics)\n",
    "\n",
    "    # # Calculate average metrics\n",
    "    # unet_avg_metrics = {}\n",
    "    # for key in unet_metrics[0]:\n",
    "    #     unet_avg_metrics[key] = np.mean([m[key] for m in unet_metrics])\n",
    "\n",
    "    # unet_avg_metrics['SSIM'] = average_ssim\n",
    "    # # unet_avg_metrics['PSNR'] = average_psnr\n",
    "\n",
    "    # print(\"\\n=== UNet Evaluation Metrics ===\")\n",
    "    # for key, value in unet_avg_metrics.items():\n",
    "    #     print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # if t == 0:\n",
    "    #   np.save(f'{metric_dir}{method}_unet_metrics_{sensor_num}.npy', unet_avg_metrics)\n",
    "    #   np.save(f'{pred_dir}{method}_unet_base_{sensor_num}_y_predict.npy', pred_test_restored)\n",
    "    #   print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "    # elif t == 1:\n",
    "    #   np.save(f'{metric_dir}{method}_unet_perturbed_metrics_{sensor_num}.npy', unet_avg_metrics)\n",
    "    #   np.save(f'{pred_dir}{method}_unet_base_{sensor_num}_y_perturbed_predict.npy', pred_test_restored)\n",
    "    #   print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCNv1EmoHyqj"
   },
   "source": [
    "### ViTAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKdxWt63H5Vs"
   },
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etru8bD2H091"
   },
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "# 2D position encoding function (converted from pos_embed.py)\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int or tuple, representing grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim]\n",
    "    \"\"\"\n",
    "    if isinstance(grid_size, int):\n",
    "        grid_h_size = grid_w_size = grid_size\n",
    "    elif isinstance(grid_size, (tuple, list)):\n",
    "        grid_h_size, grid_w_size = grid_size\n",
    "\n",
    "    grid_h = np.arange(grid_h_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_w_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # w first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_h_size, grid_w_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # Use half dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: Output dimension for each position\n",
    "    pos: List of positions to encode: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "# TensorFlow version of ViTAE components\n",
    "@register_keras_serializable()\n",
    "class PatchEmbed(layers.Layer):\n",
    "    \"\"\"Split image into patches and apply linear projection\"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        if isinstance(img_size, int):\n",
    "            img_size = (img_size, img_size)\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size, padding='valid')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Save input batch size\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Apply projection\n",
    "        x = self.proj(x)\n",
    "        # Use tf.shape instead of x.shape to get current size, and explicitly specify flattened dimensions\n",
    "        hw = self.grid_size[0] * self.grid_size[1]\n",
    "        # Reshape to [batch_size, num_patches, embed_dim]\n",
    "        x = tf.reshape(x, [batch_size, hw, self.embed_dim])\n",
    "        return x\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MLPBlock(layers.Layer):\n",
    "    \"\"\"MLP module for Transformer blocks\"\"\"\n",
    "    def __init__(self, hidden_dim, mlp_ratio=4.0, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(int(hidden_dim * mlp_ratio))\n",
    "        self.gelu = lambda x: x * tf.sigmoid(1.702 * x)  # GELU approximation\n",
    "        self.fc2 = layers.Dense(hidden_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x\n",
    "\n",
    "@register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Transformer block implementation\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=True, dropout_rate=0.0, attn_dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=dim//num_heads,\n",
    "            dropout=attn_dropout_rate, use_bias=qkv_bias\n",
    "        )\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = MLPBlock(dim, mlp_ratio, dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x_norm = self.norm1(x)\n",
    "        # For MultiHeadAttention, we need to explicitly specify query/key/value are all x_norm\n",
    "        attn_output = self.attn(x_norm, x_norm, x_norm, training=training)\n",
    "        x = x + attn_output  # Residual connection\n",
    "        x = x + self.mlp(self.norm2(x), training=training)  # Another residual connection\n",
    "        return x\n",
    "\n",
    "@register_keras_serializable()\n",
    "class CNNDecBlock(layers.Layer):\n",
    "    \"\"\"CNN decoder block\"\"\"\n",
    "    def __init__(self, out_chans, norm_layer=layers.BatchNormalization):\n",
    "        super().__init__()\n",
    "        self.conv = layers.Conv2D(out_chans, kernel_size=3, padding='same')\n",
    "        self.norm = norm_layer() if norm_layer is not None else None\n",
    "        self.act = layers.LeakyReLU(0.02)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv(x)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x, training=training)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ViTAutoEncoder(tf.keras.Model):\n",
    "    \"\"\"Vision Transformer Autoencoder implementation\"\"\"\n",
    "    def __init__(\n",
    "        self, input_size, in_chans, patch_size,\n",
    "        enc_chans=1, enc_dim=128, enc_depth=8, enc_num_heads=8,\n",
    "        enc_mlp_ratio=4., dec_dims=[16, 16, 16, 16, 16], **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.img_size = input_size\n",
    "        # self.input_size = input_size\n",
    "        # self.grid_size = (\n",
    "        #     input_size[0] // patch_size,\n",
    "        #     input_size[1] // patch_size\n",
    "        # )\n",
    "        self.in_chans = in_chans\n",
    "        self.patch_size = patch_size\n",
    "        self.enc_chans = enc_chans\n",
    "\n",
    "        # ViT encoder part\n",
    "        self.in_chans = in_chans\n",
    "        self.enc_chans = enc_chans\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        if isinstance(input_size, int):\n",
    "            self.grid_size = (input_size // patch_size, input_size // patch_size)\n",
    "        elif isinstance(input_size, (tuple, list)):\n",
    "            self.grid_size = (input_size[0] // patch_size, input_size[1] // patch_size)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(input_size, patch_size, in_chans, enc_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        pos_embed = get_2d_sincos_pos_embed(enc_dim, self.grid_size, cls_token=False)\n",
    "        self.pos_embed = tf.Variable(initial_value=pos_embed.reshape(1, num_patches, enc_dim),\n",
    "                                     trainable=False, dtype=tf.float32, name='pos_embed')\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = [\n",
    "            TransformerBlock(\n",
    "                dim=enc_dim, num_heads=enc_num_heads,\n",
    "                mlp_ratio=enc_mlp_ratio, qkv_bias=True,\n",
    "                dropout_rate=0.0, attn_dropout_rate=0.0\n",
    "            ) for _ in range(enc_depth)\n",
    "        ]\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.encoder_out = layers.Conv2D(1, kernel_size=1, padding='valid')\n",
    "\n",
    "        # CNN decoder part\n",
    "        self.decoder_embed = layers.Dense(patch_size * patch_size * enc_chans)\n",
    "\n",
    "        # Decoder CNN layers\n",
    "        dec_dims = [enc_chans] + dec_dims\n",
    "        self.decoder_cnn_blocks = [\n",
    "            CNNDecBlock(dec_dims[i+1])\n",
    "            for i in range(len(dec_dims)-1)\n",
    "        ]\n",
    "        self.decoder_out = layers.Conv2D(2, kernel_size=1, padding='valid')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_size\": self.img_size,  # Original size/patch size => grid_size\n",
    "            \"in_chans\": self.in_chans,\n",
    "            \"patch_size\": self.patch_size,\n",
    "            \"enc_chans\": self.enc_chans,\n",
    "            \"enc_dim\": self.patch_embed.embed_dim,\n",
    "            \"enc_depth\": len(self.blocks),\n",
    "            \"enc_num_heads\": self.blocks[0].attn.num_heads,\n",
    "            \"enc_mlp_ratio\": self.blocks[0].mlp.fc1.units / self.blocks[0].mlp.fc2.units,\n",
    "            \"dec_dims\": [block.conv.filters for block in self.decoder_cnn_blocks]\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Convert tokens back to image format\"\"\"\n",
    "        ph, pw = self.patch_size, self.patch_size\n",
    "        h, w = self.grid_size\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # [B, h*w, ph*pw*C] -> [B, h, w, ph, pw, C]\n",
    "        x = tf.reshape(x, [batch_size, h, w, ph, pw, self.enc_chans])\n",
    "        # [B, h, w, ph, pw, C] -> [B, C, h, ph, w, pw]\n",
    "        x = tf.transpose(x, [0, 5, 1, 3, 2, 4])\n",
    "        # [B, C, h, ph, w, pw] -> [B, C, h*ph, w*pw]\n",
    "        h_full = h * ph\n",
    "        w_full = w * pw\n",
    "        imgs = tf.reshape(x, [batch_size, self.enc_chans, h_full, w_full])\n",
    "        # [B, C, H, W] -> [B, H, W, C]\n",
    "        imgs = tf.transpose(imgs, [0, 2, 3, 1])\n",
    "        return imgs\n",
    "\n",
    "    def forward_encoder(self, x, training=False):\n",
    "        # Embed patches\n",
    "        x = self.patch_embed(x)\n",
    "        # Add positional encoding - ensure pos_embed is broadcast to correct batch size\n",
    "        x = x + self.pos_embed  # self.pos_embed is already [1, num_patches, dim], will auto-broadcast\n",
    "        # Apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, training=training)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_decoder(self, x, training=False):\n",
    "        # Embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        # Reshape to 2D\n",
    "        x_enc = self.unpatchify(x)\n",
    "        pred_enc = self.encoder_out(x_enc)\n",
    "\n",
    "        # 2D convolution layers\n",
    "        x_dec = x_enc\n",
    "        for block in self.decoder_cnn_blocks:\n",
    "            x_dec = block(x_dec, training=training)\n",
    "        pred_dec = self.decoder_out(x_dec)\n",
    "\n",
    "        return pred_dec, pred_enc\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        latent = self.forward_encoder(inputs, training=training)\n",
    "        pred_dec, pred_enc = self.forward_decoder(latent, training=training)\n",
    "        # Only return pred_dec as main output, simplifying post-processing of model predictions\n",
    "        return pred_dec\n",
    "\n",
    "def build_vitae(input_shape, model_size='base'):\n",
    "    \"\"\"Build ViTAE model, similar to original PyTorch implementation model factory function\"\"\"\n",
    "    # Extract input dimensions\n",
    "    in_h, in_w, in_c = input_shape\n",
    "    patch_size = 3  # Set appropriate patch size\n",
    "\n",
    "    if model_size == 'lite':\n",
    "        model = ViTAutoEncoder(\n",
    "            input_size=(in_h, in_w), in_chans=in_c, patch_size=patch_size,\n",
    "            enc_chans=16, enc_dim=32, enc_depth=8, enc_num_heads=8,\n",
    "            enc_mlp_ratio=4, dec_dims=[16, 16, 16, 16, 16],\n",
    "        )\n",
    "    elif model_size == 'base':\n",
    "        model = ViTAutoEncoder(\n",
    "            input_size=(in_h, in_w), in_chans=in_c, patch_size=patch_size,\n",
    "            enc_chans=32, enc_dim=64, enc_depth=8, enc_num_heads=8,\n",
    "            enc_mlp_ratio=4, dec_dims=[32, 32, 32, 32, 32],\n",
    "        )\n",
    "    elif model_size == 'large':\n",
    "        model = ViTAutoEncoder(\n",
    "            input_size=(in_h, in_w), in_chans=in_c, patch_size=patch_size,\n",
    "            enc_chans=64, enc_dim=128, enc_depth=8, enc_num_heads=8,\n",
    "            enc_mlp_ratio=4, dec_dims=[64, 64, 64, 64, 64],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model size: {model_size}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnO_jAAlLJz8"
   },
   "source": [
    "#### For Loop Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0l70a5KLQNy"
   },
   "outputs": [],
   "source": [
    "sensor_list = [5,10,15,20,25,30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test = load_my_data(sensor_num, method=method)\n",
    "  print(f'Dataset for {sensor_num} is ready.')\n",
    "  input_shape = (15, 15, 3)\n",
    "  vitae_model = build_vitae(input_shape, model_size='base')\n",
    "  vitae_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "  save_dir = model_dir\n",
    "  \n",
    "#   # Train\n",
    "#   # Training parameters\n",
    "#   batch_size = 8\n",
    "#   epochs = 100\n",
    "\n",
    "#   # Set callback functions (save best model)\n",
    "#   model_checkpoint = ModelCheckpoint(\n",
    "#       filepath=f'{save_dir}{method}_ViTAE_base-ImageReconstruction_{sensor_num}.keras',\n",
    "#       monitor='val_loss', save_best_only=True, verbose=1, mode='min'\n",
    "#   )\n",
    "#   reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5)\n",
    "#   early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "#   # Train model\n",
    "#   history = vitae_model.fit(\n",
    "#       x_train_input, y_train_output,\n",
    "#       epochs=epochs,\n",
    "#       batch_size=batch_size,\n",
    "#       validation_data=(x_val_input, y_val_output),\n",
    "#       callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
    "#       shuffle=True\n",
    "#   )\n",
    "\n",
    "  # Load\n",
    "  model_path = f'{save_dir}{method}_ViTAE_base-ImageReconstruction_{sensor_num}.keras'\n",
    "  vitae_model = load_model(model_path)\n",
    "\n",
    "  for t in range(2):\n",
    "    if t == 0:\n",
    "      # Predict test set\n",
    "      start_time = time.time()\n",
    "      pred_test = vitae_model.predict(x_test_input)\n",
    "      end_time = time.time()\n",
    "      print(f\"inference time: {end_time - start_time:.2f} seconds\")\n",
    "    # elif t == 1:\n",
    "    #   pred_test = vitae_model.predict(x_perturbed_test_input)\n",
    "\n",
    "  #   # Denormalize (restore original data scale)\n",
    "  #   pred_test_restored = restore_original_scale(pred_test, min_vals, max_vals)\n",
    "\n",
    "  #   # Calculate SSIM and PSNR scores\n",
    "  #   ssim_scores = []\n",
    "  #   # psnr_scores = []\n",
    "  #   for i in range(len(pred_test_restored)):\n",
    "  #       # Calculate SSIM for U and V components separately\n",
    "  #       ssim_u = ssim(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(), multichannel=False)\n",
    "  #       ssim_v = ssim(pred_test_restored[i, :, :, 1], y_test[i, :, :, 1], data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(), multichannel=False)\n",
    "  #       ssim_score = (ssim_u + ssim_v) / 2\n",
    "  #       # psnr_score = psnr(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "  #       ssim_scores.append(ssim_score)\n",
    "  #       # psnr_scores.append(psnr_score)\n",
    "\n",
    "  #   # Calculate average SSIM and PSNR\n",
    "  #   average_ssim = np.mean(ssim_scores)\n",
    "  #   # average_psnr = np.mean(psnr_scores)\n",
    "\n",
    "  #   vitae_metrics = []\n",
    "  #   for i in range(len(pred_test_restored)):\n",
    "  #       # Calculate for U and V components separately\n",
    "  #       u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_test_restored[i, :, :, 0])\n",
    "  #       v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_test_restored[i, :, :, 1])\n",
    "\n",
    "  #       # Combine U and V metrics (take average)\n",
    "  #       combined_metrics = {}\n",
    "  #       for key in u_metrics:\n",
    "  #           combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "  #       vitae_metrics.append(combined_metrics)\n",
    "\n",
    "  #   # Calculate average metrics\n",
    "  #   vitae_avg_metrics = {}\n",
    "  #   for key in vitae_metrics[0]:\n",
    "  #       vitae_avg_metrics[key] = np.mean([m[key] for m in vitae_metrics])\n",
    "\n",
    "  #   vitae_avg_metrics['SSIM'] = average_ssim\n",
    "  #   # vitae_avg_metrics['PSNR'] = average_psnr\n",
    "\n",
    "  #   print(\"\\n=== Kriging Additional Evaluation Metrics ===\")\n",
    "  #   for key, value in vitae_avg_metrics.items():\n",
    "  #       print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "  #   if t == 0:\n",
    "  #     np.save(f'{metric_dir}{method}_vitae_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "  #     np.save(f'{pred_dir}{method}_vitae_base_{sensor_num}_y_predict.npy', pred_test_restored)\n",
    "  #     print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "  #   elif t == 1:\n",
    "  #     np.save(f'{metric_dir}{method}_vitae_perturbed_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "  #     np.save(f'{pred_dir}{method}_vitae_base_{sensor_num}_y_perturbed_predict.npy', pred_test_restored)\n",
    "  #     print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = [5,10,15,20,25,30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test = load_my_data(sensor_num, method=1)\n",
    "  print(f'Dataset for {sensor_num} is ready.')\n",
    "  input_shape = (15, 15, 3)\n",
    "  vitae_model = build_vitae(input_shape, model_size='base')\n",
    "  vitae_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "  save_dir = model_dir\n",
    "  \n",
    "  # Train\n",
    "  # Training parameters\n",
    "  batch_size = 8\n",
    "  epochs = 100\n",
    "\n",
    "  # Set callback functions (save best model)\n",
    "  model_checkpoint = ModelCheckpoint(\n",
    "      filepath=f'{save_dir}1_ViTAE_base-ImageReconstruction_{sensor_num}.keras',\n",
    "      monitor='val_loss', save_best_only=True, verbose=1, mode='min'\n",
    "  )\n",
    "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5)\n",
    "  early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "  # Train model\n",
    "  history = vitae_model.fit(\n",
    "      x_train_input, y_train_output,\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "      validation_data=(x_val_input, y_val_output),\n",
    "      callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
    "      shuffle=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model was lost due to incorrect model dir setting during training, but metrics can be calculated using correctly saved pred\n",
    "sensor_list = [5,10,15,20,25,30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  for t in range(2):\n",
    "    if t == 0:\n",
    "      # Predict test set\n",
    "      pred_test_restored= np.load(f'{pred_dir}{method}_vitae_base_{sensor_num}_y_predict.npy')\n",
    "    elif t == 1:\n",
    "      pred_test_restored = np.load(f'{pred_dir}{method}_vitae_base_{sensor_num}_y_perturbed_predict.npy')\n",
    "\n",
    "        # Calculate SSIM and PSNR scores\n",
    "    ssim_scores = []\n",
    "    # psnr_scores = []\n",
    "    for i in range(len(pred_test_restored)):\n",
    "        # Calculate SSIM for U and V components separately\n",
    "        ssim_u = ssim(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(), multichannel=False)\n",
    "        ssim_v = ssim(pred_test_restored[i, :, :, 1], y_test[i, :, :, 1], data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(), multichannel=False)\n",
    "        ssim_score = (ssim_u + ssim_v) / 2\n",
    "        # psnr_score = psnr(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "        ssim_scores.append(ssim_score)\n",
    "        # psnr_scores.append(psnr_score)\n",
    "\n",
    "    # Calculate average SSIM and PSNR\n",
    "    average_ssim = np.mean(ssim_scores)\n",
    "    # average_psnr = np.mean(psnr_scores)\n",
    "\n",
    "    vitae_metrics = []\n",
    "    for i in range(len(pred_test_restored)):\n",
    "        # Calculate for U and V components separately\n",
    "        u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_test_restored[i, :, :, 0])\n",
    "        v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_test_restored[i, :, :, 1])\n",
    "\n",
    "        # Combine U and V metrics (take average)\n",
    "        combined_metrics = {}\n",
    "        for key in u_metrics:\n",
    "            combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "        vitae_metrics.append(combined_metrics)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    vitae_avg_metrics = {}\n",
    "    for key in vitae_metrics[0]:\n",
    "        vitae_avg_metrics[key] = np.mean([m[key] for m in vitae_metrics])\n",
    "\n",
    "    vitae_avg_metrics['SSIM'] = average_ssim\n",
    "    # vitae_avg_metrics['PSNR'] = average_psnr\n",
    "\n",
    "    print(\"\\n=== ViTAE Additional Evaluation Metrics ===\")\n",
    "    for key, value in vitae_avg_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    if t == 0:\n",
    "      np.save(f'{metric_dir}{method}_vitae_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "      print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "    elif t == 1:\n",
    "      np.save(f'{metric_dir}{method}_vitae_perturbed_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "      print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryIERD5r1fWG"
   },
   "source": [
    "### Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZN-CiTIzTkP"
   },
   "source": [
    "#### For Loop Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_MjNPO10tIA"
   },
   "outputs": [],
   "source": [
    "sensor_list = [5,10,15,20,25,30]\n",
    "#sensor_list = [30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  x_train_input, y_train_output, x_val_input, y_val_output, x_test_input, x_perturbed_test_input, y_test_output, min_vals, max_vals, y_test, x_test = load_my_data(sensor_num, method=method)\n",
    "  print(f'Dataset for {sensor_num} is ready.')\n",
    "\n",
    "  for t in range(2):\n",
    "    if t == 0:\n",
    "      start_time = time.time()\n",
    "      pred_test = apply_kriging_reconstruction(x_test_input, y_test_output, method='ordinary')\n",
    "      end_time = time.time()\n",
    "      print(f\"inference time: {end_time - start_time:.2f} seconds\")\n",
    "    # elif t == 1:\n",
    "    #   pred_test = apply_kriging_reconstruction(x_perturbed_test_input, y_test_output, method='ordinary')\n",
    "\n",
    "    # pred_kriging = restore_original_scale(pred_test, min_vals, max_vals)\n",
    "\n",
    "\n",
    "    # # Calculate SSIM and PSNR scores\n",
    "    # kriging_ssim_scores = []\n",
    "    # # kriging_psnr_scores = []\n",
    "    # for i in range(len(pred_kriging)):\n",
    "    #   # Calculate SSIM for U and V components separately\n",
    "    #   ssim_u = ssim(pred_kriging[i, :, :, 0], y_test[i, :, :, 0],\n",
    "    #                 data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(),\n",
    "    #                 multichannel=False)\n",
    "    #   ssim_v = ssim(pred_kriging[i, :, :, 1], y_test[i, :, :, 1],\n",
    "    #                 data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(),\n",
    "    #                 multichannel=False)\n",
    "    #   ssim_score = (ssim_u + ssim_v) / 2\n",
    "    #   # psnr_score = psnr(pred_kriging[i, :, :, 0], y_test[i, :, :, 0],\n",
    "    #   #                   data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "    #   kriging_ssim_scores.append(ssim_score)\n",
    "    #   # kriging_psnr_scores.append(psnr_score)\n",
    "\n",
    "    # # Calculate average SSIM and PSNR\n",
    "    # kriging_average_ssim = np.mean(kriging_ssim_scores)\n",
    "    # # kriging_average_psnr = np.mean(kriging_psnr_scores)\n",
    "\n",
    "    # # Calculate additional metrics for Kriging\n",
    "    # kriging_metrics = []\n",
    "    # for i in range(len(pred_kriging)):\n",
    "    #     # Calculate for U and V components separately\n",
    "    #     u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_kriging[i, :, :, 0])\n",
    "    #     v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_kriging[i, :, :, 1])\n",
    "\n",
    "    #     # Combine U and V metrics (take average)\n",
    "    #     combined_metrics = {}\n",
    "    #     for key in u_metrics:\n",
    "    #         combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "    #     kriging_metrics.append(combined_metrics)\n",
    "\n",
    "    # # Calculate average metrics\n",
    "    # kriging_avg_metrics = {}\n",
    "    # for key in kriging_metrics[0]:\n",
    "    #     kriging_avg_metrics[key] = np.mean([m[key] for m in kriging_metrics])\n",
    "\n",
    "    # kriging_avg_metrics['SSIM'] = kriging_average_ssim\n",
    "    # # kriging_avg_metrics['PSNR'] = kriging_average_psnr\n",
    "\n",
    "    # # print(\"\\n=== Kriging Additional Evaluation Metrics ===\")\n",
    "    # # for key, value in kriging_avg_metrics.items():\n",
    "    # #     print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # if t == 0:\n",
    "    #   np.save(f'{metric_dir}{method}_kriging_metrics_{sensor_num}.npy', kriging_avg_metrics)\n",
    "    #   np.save(f'{pred_dir}{method}_kriging_base_{sensor_num}_y_predict.npy', pred_kriging)\n",
    "    #   print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "    # elif t == 1:\n",
    "    #   np.save(f'{metric_dir}{method}_kriging_perturbed_metrics_{sensor_num}.npy', kriging_avg_metrics)\n",
    "    #   np.save(f'{pred_dir}{method}_kriging_base_{sensor_num}_y_perturbed_predict.npy', pred_kriging)\n",
    "    #   print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily calculate fac2\n",
    "# Model was lost due to incorrect model dir setting during training, but metrics can be calculated using correctly saved pred\n",
    "sensor_list = [5,10,15,20,25,30]\n",
    "for s in range(len(sensor_list)):\n",
    "  sensor_num = sensor_list[s]\n",
    "  for t in range(2):\n",
    "    if t == 0:\n",
    "      # Predict test set\n",
    "      pred_test_restored= np.load(f'{pred_dir}{method}_kriging_base_{sensor_num}_y_predict.npy')\n",
    "    elif t == 1:\n",
    "      pred_test_restored = np.load(f'{pred_dir}{method}_kriging_base_{sensor_num}_y_perturbed_predict.npy')\n",
    "\n",
    "        # Calculate SSIM and PSNR scores\n",
    "    ssim_scores = []\n",
    "    # psnr_scores = []\n",
    "    for i in range(len(pred_test_restored)):\n",
    "        # Calculate SSIM for U and V components separately\n",
    "        ssim_u = ssim(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min(), multichannel=False)\n",
    "        ssim_v = ssim(pred_test_restored[i, :, :, 1], y_test[i, :, :, 1], data_range=y_test[i, :, :, 1].max() - y_test[i, :, :, 1].min(), multichannel=False)\n",
    "        ssim_score = (ssim_u + ssim_v) / 2\n",
    "        # psnr_score = psnr(pred_test_restored[i, :, :, 0], y_test[i, :, :, 0], data_range=y_test[i, :, :, 0].max() - y_test[i, :, :, 0].min())\n",
    "        ssim_scores.append(ssim_score)\n",
    "        # psnr_scores.append(psnr_score)\n",
    "\n",
    "    # Calculate average SSIM and PSNR\n",
    "    average_ssim = np.mean(ssim_scores)\n",
    "    # average_psnr = np.mean(psnr_scores)\n",
    "\n",
    "    vitae_metrics = []\n",
    "    for i in range(len(pred_test_restored)):\n",
    "        # Calculate for U and V components separately\n",
    "        u_metrics = calculate_all_metrics(y_test[i, :, :, 0], pred_test_restored[i, :, :, 0])\n",
    "        v_metrics = calculate_all_metrics(y_test[i, :, :, 1], pred_test_restored[i, :, :, 1])\n",
    "\n",
    "        # Combine U and V metrics (take average)\n",
    "        combined_metrics = {}\n",
    "        for key in u_metrics:\n",
    "            combined_metrics[key] = (u_metrics[key] + v_metrics[key]) / 2\n",
    "\n",
    "        vitae_metrics.append(combined_metrics)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    vitae_avg_metrics = {}\n",
    "    for key in vitae_metrics[0]:\n",
    "        vitae_avg_metrics[key] = np.mean([m[key] for m in vitae_metrics])\n",
    "\n",
    "    vitae_avg_metrics['SSIM'] = average_ssim\n",
    "    # vitae_avg_metrics['PSNR'] = average_psnr\n",
    "\n",
    "    print(\"\\n=== Kriging Additional Evaluation Metrics ===\")\n",
    "    for key, value in vitae_avg_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    if t == 0:\n",
    "      np.save(f'{metric_dir}{method}_kriging_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "      print(f\"Saved normal test set results: sensor_num={sensor_num}\")\n",
    "    elif t == 1:\n",
    "      np.save(f'{metric_dir}{method}_kriging_perturbed_metrics_{sensor_num}.npy', vitae_avg_metrics)\n",
    "      print(f\"Saved perturbed test set results: sensor_num={sensor_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugdi3Mi_9tKm"
   },
   "source": [
    "# QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHkQPh189xuE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import qr\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_pod_basis(y_data, r=10):\n",
    "    \"\"\"\n",
    "    Build POD basis from flow field data\n",
    "\n",
    "    Parameters:\n",
    "        y_data: (N, 15, 15, 2) - Flow field data, u and v components\n",
    "        r: Number of POD modes to retain\n",
    "\n",
    "    Returns:\n",
    "        Psi_r: (450, r) - POD basis matrix\n",
    "        explained_variance: Energy proportion of first r modes\n",
    "    \"\"\"\n",
    "    N, H, W, C = y_data.shape\n",
    "\n",
    "    # Reshape data to (450, N) - each row is a time series for one position\n",
    "    data_matrix = y_data.reshape(N, H*W*C).T  # (450, N)\n",
    "\n",
    "    # Center the data\n",
    "    data_mean = np.mean(data_matrix, axis=1, keepdims=True)\n",
    "    data_centered = data_matrix - data_mean\n",
    "\n",
    "    # SVD decomposition to get POD modes\n",
    "    U, S, Vt = np.linalg.svd(data_centered, full_matrices=False)\n",
    "\n",
    "    # Calculate energy proportion\n",
    "    total_energy = np.sum(S**2)\n",
    "    explained_variance = np.cumsum(S**2) / total_energy\n",
    "\n",
    "    # Select first r modes\n",
    "    Psi_r = U[:, :r]\n",
    "\n",
    "    print(f\"First {r} modes explain {explained_variance[r-1]:.4f} of total energy\")\n",
    "\n",
    "    return Psi_r, explained_variance[:r]\n",
    "\n",
    "def qr_sensor_ranking(Psi_r):\n",
    "    \"\"\"\n",
    "    Rank all sensor positions by importance using QR decomposition\n",
    "\n",
    "    Parameters:\n",
    "        Psi_r: (450, r) - POD basis matrix\n",
    "\n",
    "    Returns:\n",
    "        ranking_results: Dictionary containing complete ranking information\n",
    "    \"\"\"\n",
    "    # QR decomposition with column pivoting\n",
    "    Q, R, P = qr(Psi_r.T, pivoting=True)\n",
    "\n",
    "        # P is the importance ranking! P[0] is the most important position index\n",
    "    print(f\"QR decomposition completed, total {len(P)} positions\")\n",
    "    \n",
    "    # Convert indices to spatial position information\n",
    "    ranking_results = {\n",
    "        'global_ranking': P,\n",
    "        'spatial_info': [],\n",
    "        'importance_scores': []\n",
    "    }\n",
    "\n",
    "    for rank, idx in enumerate(P):\n",
    "        # Convert index to spatial coordinates\n",
    "        spatial_idx = idx // 2  # Spatial position index (0-224)\n",
    "        component = idx % 2     # Component index (0=u, 1=v)\n",
    "\n",
    "        row = spatial_idx // 15  # Row coordinate\n",
    "        col = spatial_idx % 15   # Column coordinate\n",
    "\n",
    "        # Calculate importance score (normalized to 0-1)\n",
    "        importance_score = (len(P) - rank) / len(P)\n",
    "\n",
    "        spatial_info = {\n",
    "            'global_rank': rank + 1,\n",
    "            'position': (row, col),\n",
    "            'component': 'u' if component == 0 else 'v',\n",
    "            'importance_score': importance_score,\n",
    "            'percentile': (len(P) - rank) / len(P) * 100\n",
    "        }\n",
    "\n",
    "        ranking_results['spatial_info'].append(spatial_info)\n",
    "        ranking_results['importance_scores'].append(importance_score)\n",
    "\n",
    "    return ranking_results\n",
    "\n",
    "def analyze_current_sensors(current_sensor_positions, ranking_results):\n",
    "    \"\"\"\n",
    "    Analyze current sensor positions in global ranking\n",
    "\n",
    "    Parameters:\n",
    "        current_sensor_positions: [(row, col, component), ...] - Current sensor positions\n",
    "        ranking_results: QR ranking results\n",
    "\n",
    "    Returns:\n",
    "        sensor_analysis: Detailed analysis of current sensors\n",
    "    \"\"\"\n",
    "    sensor_analysis = []\n",
    "\n",
    "    for sensor_pos in current_sensor_positions:\n",
    "        row, col, comp = sensor_pos\n",
    "\n",
    "        # Find this sensor in ranking results\n",
    "        found = False\n",
    "        for spatial_info in ranking_results['spatial_info']:\n",
    "            if (spatial_info['position'] == (row, col) and\n",
    "                spatial_info['component'] == comp):\n",
    "\n",
    "                analysis = {\n",
    "                    'sensor_position': sensor_pos,\n",
    "                    'global_rank': spatial_info['global_rank'],\n",
    "                    'importance_score': spatial_info['importance_score'],\n",
    "                    'percentile': spatial_info['percentile'],\n",
    "                    'category': categorize_importance(spatial_info['global_rank'])\n",
    "                }\n",
    "                sensor_analysis.append(analysis)\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            print(f\"Warning: Sensor position {sensor_pos} not found\")\n",
    "    \n",
    "    # Sort by importance\n",
    "    sensor_analysis.sort(key=lambda x: x['global_rank'])\n",
    "\n",
    "    return sensor_analysis\n",
    "\n",
    "def categorize_importance(rank, total=450):\n",
    "    \"\"\"Categorize sensor importance\"\"\"\n",
    "    percentile = rank / total * 100\n",
    "\n",
    "    if percentile <= 10:\n",
    "        return \"Critical (Top 10%)\"\n",
    "    elif percentile <= 30:\n",
    "        return \"Important (Top 30%)\"\n",
    "    elif percentile <= 60:\n",
    "        return \"Moderate (Top 60%)\"\n",
    "    else:\n",
    "        return \"Low (Bottom 40%)\"\n",
    "\n",
    "def extract_current_sensor_positions(x_data):\n",
    "    \"\"\"\n",
    "    Extract current sensor positions from third channel of input data\n",
    "\n",
    "    Parameters:\n",
    "        x_data: (N, 15, 15, 3) - Input data, third channel is sensor mask\n",
    "\n",
    "    Returns:\n",
    "        sensor_positions: [(row, col, 'u'), (row, col, 'v'), ...] - List of sensor positions\n",
    "    \"\"\"\n",
    "    # Use sensor mask from first sample\n",
    "    sensor_mask = x_data[0, :, :, 2]\n",
    "    sensor_coords = np.where(sensor_mask == 1)\n",
    "\n",
    "    sensor_positions = []\n",
    "    for row, col in zip(sensor_coords[0], sensor_coords[1]):\n",
    "        # Assume each sensor position measures both u and v\n",
    "        sensor_positions.append((row, col, 'u'))\n",
    "        sensor_positions.append((row, col, 'v'))\n",
    "\n",
    "    return sensor_positions\n",
    "\n",
    "def visualize_sensor_importance(ranking_results, current_sensors=None, top_k=20):\n",
    "    \"\"\"\n",
    "    Visualize sensor importance distribution\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # 1. Importance heatmap\n",
    "    importance_map_u = np.zeros((15, 15))\n",
    "    importance_map_v = np.zeros((15, 15))\n",
    "\n",
    "    for info in ranking_results['spatial_info']:\n",
    "        row, col = info['position']\n",
    "        score = info['importance_score']\n",
    "\n",
    "        if info['component'] == 'u':\n",
    "            importance_map_u[row, col] = score\n",
    "        else:\n",
    "            importance_map_v[row, col] = score\n",
    "\n",
    "    im1 = ax1.imshow(importance_map_u, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax1.set_title('U Component Importance')\n",
    "    ax1.set_xlabel('Column')\n",
    "    ax1.set_ylabel('Row')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    # Mark current sensor positions\n",
    "    if current_sensors:\n",
    "        for sensor in current_sensors:\n",
    "            if sensor[2] == 'u':\n",
    "                ax1.scatter(sensor[1], sensor[0], c='red', s=100, marker='x')\n",
    "\n",
    "    im2 = ax2.imshow(importance_map_v, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax2.set_title('V Component Importance')\n",
    "    ax2.set_xlabel('Column')\n",
    "    ax2.set_ylabel('Row')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "    # Mark current sensor positions\n",
    "    if current_sensors:\n",
    "        for sensor in current_sensors:\n",
    "            if sensor[2] == 'v':\n",
    "                ax2.scatter(sensor[1], sensor[0], c='red', s=100, marker='x')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Top-k important positions list\n",
    "    print(f\"\\n=== Top {top_k} Most Important Sensor Positions ===\")\n",
    "    for i in range(min(top_k, len(ranking_results['spatial_info']))):\n",
    "        info = ranking_results['spatial_info'][i]\n",
    "        print(f\"Rank {info['global_rank']:3d}: Position {info['position']}, \"\n",
    "              f\"Component {info['component']}, Score {info['importance_score']:.4f}\")\n",
    "\n",
    "def main_qr_analysis(sensor_num=5):\n",
    "    \"\"\"\n",
    "    Main analysis function\n",
    "    \"\"\"\n",
    "    print(f\"=== QR Sensor Position Analysis (sensor_num={sensor_num}) ===\\n\")\n",
    "\n",
    "    # 1. Load data\n",
    "    out_dir = '/content/drive/MyDrive/TorchDA/dataset'\n",
    "    x1_data = np.load(f'{out_dir}/x1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "    y1_data = np.load(f'{out_dir}/y1_data_{sensor_num}.npy', mmap_mode='r')\n",
    "    x2_data = np.load(f'{out_dir}/x2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "    y2_data = np.load(f'{out_dir}/y2_data_{sensor_num}.npy', mmap_mode='r')\n",
    "\n",
    "    # Combine training data\n",
    "    y_train = np.concatenate([y1_data, y2_data], axis=0)\n",
    "    x_train = np.concatenate([x1_data, x2_data], axis=0)\n",
    "\n",
    "    print(f\"Training data shape: {y_train.shape}\")\n",
    "\n",
    "    # 2. Build POD basis\n",
    "    print(\"\\n=== Step 1: Build POD Basis ===\")\n",
    "    Psi_r, explained_var = build_pod_basis(y_train, r=40)\n",
    "\n",
    "    # 3. QR ranking analysis\n",
    "    print(\"\\n=== Step 2: QR Sensor Position Ranking ===\")\n",
    "    ranking_results = qr_sensor_ranking(Psi_r)\n",
    "\n",
    "    # 4. Analyze current sensor positions\n",
    "    print(\"\\n=== Step 3: Analyze Current Sensor Positions ===\")\n",
    "    current_sensors = extract_current_sensor_positions(x_train)\n",
    "    sensor_analysis = analyze_current_sensors(current_sensors, ranking_results)\n",
    "\n",
    "    print(f\"Current sensor analysis results:\")\n",
    "    for analysis in sensor_analysis:\n",
    "        print(f\"Sensor {analysis['sensor_position']}: \"\n",
    "              f\"Global rank #{analysis['global_rank']}, \"\n",
    "              f\"Importance score {analysis['importance_score']:.4f}, \"\n",
    "              f\"Category: {analysis['category']}\")\n",
    "\n",
    "    # 5. Visualization\n",
    "    print(\"\\n=== Step 4: Visualization Analysis Results ===\")\n",
    "    visualize_sensor_importance(ranking_results, current_sensors, top_k=20)\n",
    "\n",
    "    return ranking_results, sensor_analysis\n",
    "\n",
    "# # Run analysis\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Analyze case with 5 sensors\n",
    "#     ranking_results, sensor_analysis = main_qr_analysis(sensor_num=5)\n",
    "\n",
    "    # Can also batch analyze different sensor numbers\n",
    "    # for sensor_num in [5, 10, 15, 20, 25, 30]:\n",
    "    #     print(f\"\\n{'='*50}\")\n",
    "    #     main_qr_analysis(sensor_num=sensor_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75bCXtzn93lv"
   },
   "outputs": [],
   "source": [
    "ranking_results, sensor_analysis = main_qr_analysis(sensor_num=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lguXZgYjfLia",
    "Ou1QOLO_fLie",
    "osM-nLOQVWEO",
    "vdyVWzlnVaOP",
    "IAZFbybCVpGd",
    "dzugz-87CH7Y",
    "Qc5oaSzHB6jS",
    "4cTsblv-B-J4",
    "86H2VYI6H-mi",
    "ryIERD5r1fWG",
    "xytY7Ke0ziG1",
    "01VmdcidhJaG",
    "3-_XkyWqhXZA",
    "5pHIVlXau7D0",
    "bewHC2GQ9YdT",
    "Je4otS3xejcw",
    "taQqSeT4Kdzi",
    "Ugdi3Mi_9tKm"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
