{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750958611577,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "YnS8bCICE1W2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ-0MFo18CNt"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750958625174,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "TBQyaHOujpxw"
   },
   "outputs": [],
   "source": [
    "def generate_perturbed_test_data(X, y, grid_shape, max_perturbation=1):\n",
    "    \"\"\"\n",
    "    Generate a new test set with perturbed sensor positions based on the original test set.\n",
    "\n",
    "    Parameters:\n",
    "      X: Original test set data, shape (N, 15, 15, 3)\n",
    "      y: Original test set ground truth, shape (N, 15, 15, 2)\n",
    "      grid_shape: Grid shape, e.g., (15, 15)\n",
    "      max_perturbation: Maximum perturbation range (grid cells), default is 2\n",
    "\n",
    "    Returns:\n",
    "      X_perturbed: Test set data after sensor position perturbation\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = grid_shape\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    # Create array to store perturbed data\n",
    "    X_perturbed = np.zeros_like(X)\n",
    "\n",
    "    # Get grid coordinates (consistent with generate_data_csv function)\n",
    "    x_coords = np.linspace(0, 1, n_cols)  # Assume coordinates are in [0,1] range\n",
    "    y_coords = np.linspace(0, 1, n_rows)\n",
    "    xv, yv = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Extract sensor positions from the first sample as template\n",
    "    # We assume all samples use the same sensor position layout\n",
    "    template_mask = X[0, :, :, 2]\n",
    "    template_sensor_rows, template_sensor_cols = np.where(template_mask == 1)\n",
    "    template_sensor_positions = np.column_stack((template_sensor_rows, template_sensor_cols))\n",
    "\n",
    "    # Add random perturbation to each sensor position in the template to generate a unified perturbed template\n",
    "    perturbed_positions = np.zeros_like(template_sensor_positions)\n",
    "    for j, (row, col) in enumerate(template_sensor_positions):\n",
    "        # Generate random perturbation (between -max_perturbation and max_perturbation)\n",
    "        perturbation_row = np.random.randint(-max_perturbation, max_perturbation + 1)\n",
    "        perturbation_col = np.random.randint(-max_perturbation, max_perturbation + 1)\n",
    "\n",
    "        # Apply perturbation, ensuring position remains within grid bounds\n",
    "        new_row = np.clip(row + perturbation_row, 0, n_rows - 1)\n",
    "        new_col = np.clip(col + perturbation_col, 0, n_cols - 1)\n",
    "\n",
    "        perturbed_positions[j] = [new_row, new_col]\n",
    "\n",
    "    # Construct unified perturbed sensor mask\n",
    "    new_mask = np.zeros(grid_shape, dtype=np.float32)\n",
    "    new_mask[perturbed_positions[:, 0].astype(int), perturbed_positions[:, 1].astype(int)] = 1\n",
    "\n",
    "    print(\"Generated unified perturbed sensor mask, all test samples will share this mask\")\n",
    "\n",
    "    # Process each sample\n",
    "    for i in tqdm(range(num_samples), desc=\"Processing perturbed samples\"):\n",
    "        # Get U, V values at perturbed sensor positions\n",
    "        gt_U = y[i, :, :, 0]\n",
    "        gt_V = y[i, :, :, 1]\n",
    "\n",
    "        sensor_data_U = gt_U[perturbed_positions[:, 0].astype(int), perturbed_positions[:, 1].astype(int)]\n",
    "        sensor_data_V = gt_V[perturbed_positions[:, 0].astype(int), perturbed_positions[:, 1].astype(int)]\n",
    "\n",
    "        # Get physical coordinates corresponding to sensors\n",
    "        sensor_coords = np.column_stack((\n",
    "            xv[perturbed_positions[:, 0].astype(int), perturbed_positions[:, 1].astype(int)],\n",
    "            yv[perturbed_positions[:, 0].astype(int), perturbed_positions[:, 1].astype(int)]\n",
    "        ))\n",
    "\n",
    "        # Interpolate U and V components separately using griddata\n",
    "        grid_U = griddata(sensor_coords, sensor_data_U, (xv, yv), method='nearest')\n",
    "        grid_V = griddata(sensor_coords, sensor_data_V, (xv, yv), method='nearest')\n",
    "\n",
    "        # Save perturbed results, all samples use the same perturbed mask\n",
    "        X_perturbed[i, :, :, 0] = grid_U\n",
    "        X_perturbed[i, :, :, 1] = grid_V\n",
    "        X_perturbed[i, :, :, 2] = new_mask\n",
    "\n",
    "    return X_perturbed\n",
    "\n",
    "def visualize_perturbation(X, X_perturbed, y, sample_index=0):\n",
    "    \"\"\"\n",
    "    Visualize comparison of original and perturbed sensor positions and interpolation results.\n",
    "\n",
    "    Parameters:\n",
    "      X: Original data, shape (N, 15, 15, 3)\n",
    "      X_perturbed: Perturbed data, shape (N, 15, 15, 3)\n",
    "      y: Ground truth data, shape (N, 15, 15, 2)\n",
    "      sample_index: Index of sample to visualize, default is 0\n",
    "    \"\"\"\n",
    "    # Extract original and perturbed masks\n",
    "    original_mask = X[sample_index, :, :, 2]\n",
    "    perturbed_mask = X_perturbed[sample_index, :, :, 2]\n",
    "\n",
    "    # Extract original and perturbed U, V interpolation fields\n",
    "    original_U = X[sample_index, :, :, 0]\n",
    "    original_V = X[sample_index, :, :, 1]\n",
    "    perturbed_U = X_perturbed[sample_index, :, :, 0]\n",
    "    perturbed_V = X_perturbed[sample_index, :, :, 1]\n",
    "\n",
    "    # Extract ground truth\n",
    "    gt_U = y[sample_index, :, :, 0]\n",
    "    gt_V = y[sample_index, :, :, 1]\n",
    "\n",
    "    # Create grid coordinates (assuming 15x15 grid)\n",
    "    n_rows, n_cols = original_mask.shape\n",
    "    x_coords = np.linspace(0, 1, n_cols)\n",
    "    y_coords = np.linspace(0, 1, n_rows)\n",
    "    extent = (x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max())\n",
    "\n",
    "    # Create plot\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    # Ground Truth U\n",
    "    im0 = axs[0, 0].imshow(gt_U, origin='lower', extent=extent)\n",
    "    axs[0, 0].set_title(\"Ground Truth U\")\n",
    "    axs[0, 0].set_xlabel(\"X\")\n",
    "    axs[0, 0].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im0, ax=axs[0, 0])\n",
    "\n",
    "    # Original sensor positions and interpolated U\n",
    "    im1 = axs[0, 1].imshow(original_U, origin='lower', extent=extent)\n",
    "    axs[0, 1].set_title(\"Original Interpolated U\")\n",
    "    axs[0, 1].set_xlabel(\"X\")\n",
    "    axs[0, 1].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im1, ax=axs[0, 1])\n",
    "\n",
    "    # Perturbed sensor positions and interpolated U\n",
    "    im2 = axs[0, 2].imshow(perturbed_U, origin='lower', extent=extent)\n",
    "    axs[0, 2].set_title(\"Perturbed Interpolated U\")\n",
    "    axs[0, 2].set_xlabel(\"X\")\n",
    "    axs[0, 2].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im2, ax=axs[0, 2])\n",
    "\n",
    "    # Ground Truth V\n",
    "    im3 = axs[1, 0].imshow(gt_V, origin='lower', extent=extent)\n",
    "    axs[1, 0].set_title(\"Ground Truth V\")\n",
    "    axs[1, 0].set_xlabel(\"X\")\n",
    "    axs[1, 0].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im3, ax=axs[1, 0])\n",
    "\n",
    "    # Compare original and perturbed sensor positions\n",
    "    axs[1, 1].imshow(original_mask, origin='lower', extent=extent, alpha=0.5)\n",
    "    axs[1, 1].set_title(\"Original Sensor Positions\")\n",
    "    axs[1, 1].set_xlabel(\"X\")\n",
    "    axs[1, 1].set_ylabel(\"Y\")\n",
    "\n",
    "    axs[1, 2].imshow(perturbed_mask, origin='lower', extent=extent, alpha=0.5)\n",
    "    axs[1, 2].set_title(\"Perturbed Sensor Positions\")\n",
    "    axs[1, 2].set_xlabel(\"X\")\n",
    "    axs[1, 2].set_ylabel(\"Y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750958625198,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "dngRaXfpE1W4"
   },
   "outputs": [],
   "source": [
    "def get_uniform_sensor_positions_center(grid_shape, sensor_num):\n",
    "    \"\"\"\n",
    "    Given a 2D grid shape grid_shape (e.g., (15,15)) and the required number of sensors sensor_num,\n",
    "    first divide the grid into several regions and take the center point of each region as candidate sensor positions,\n",
    "    then uniformly sample sensor_num sensor positions from the candidates.\n",
    "\n",
    "    Parameters:\n",
    "      grid_shape: Shape of the 2D grid (n_rows, n_cols)\n",
    "      sensor_num: Required number of sensors\n",
    "\n",
    "    Returns:\n",
    "      sensor_positions: Array of shape (sensor_num, 2), each row represents [row_index, col_index]\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = grid_shape\n",
    "\n",
    "    # First estimate how many segments in row and column directions\n",
    "    # Here we take m = floor(sqrt(sensor_num)), n = ceil(sensor_num/m)\n",
    "    m = int(np.floor(np.sqrt(sensor_num)))\n",
    "    n = int(np.ceil(sensor_num / m))\n",
    "\n",
    "    # Calculate center point indices for each region:\n",
    "    # For row direction, divide [0, n_rows) into m regions, each region width is region_h = n_rows/m\n",
    "    # Center point is approximately int((i + 0.5) * region_h)\n",
    "    region_h = n_rows / m\n",
    "    region_w = n_cols / n\n",
    "    row_centers = [int((i + 0.5) * region_h) for i in range(m)]\n",
    "    col_centers = [int((j + 0.5) * region_w) for j in range(n)]\n",
    "\n",
    "    # Generate candidate sensor positions (Cartesian product)\n",
    "    candidate_positions = np.array([[r, c] for r in row_centers for c in col_centers])\n",
    "\n",
    "    # If number of candidates exceeds sensor_num, uniformly select sensor_num points from them\n",
    "    total_candidates = candidate_positions.shape[0]\n",
    "    if total_candidates > sensor_num:\n",
    "        indices = np.linspace(0, total_candidates - 1, sensor_num, dtype=int)\n",
    "        sensor_positions = candidate_positions[indices]\n",
    "    else:\n",
    "        sensor_positions = candidate_positions  # If exactly equal to or less than sensor_num\n",
    "\n",
    "    return sensor_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1750958625202,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "u4hmjsaV1WrK"
   },
   "outputs": [],
   "source": [
    "def load_optimal_sensors(method, sensor_num, sensors_dir='./optimal_sensors/'):\n",
    "    \"\"\"\n",
    "    Load optimal sensor positions for specified method and sensor_num\n",
    "\n",
    "    Parameters:\n",
    "        method: Method number (0 or 1)\n",
    "        sensor_num: Number of sensors\n",
    "        sensors_dir: Directory where files are saved\n",
    "\n",
    "    Returns:\n",
    "        positions: Sensor position array, shape: (sensor_num, 2)\n",
    "    \"\"\"\n",
    "    filename = f'optimal_sensors_method{method}_num{sensor_num}.npy'\n",
    "    filepath = os.path.join(sensors_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Optimal sensor file not found: {filepath}\")\n",
    "\n",
    "    positions = np.load(filepath)  # shape: (sensor_num, 2)\n",
    "    print(f\"Loaded optimal sensor positions: {filename}, shape: {positions.shape}\")\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750958625416,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "GjSuRSDS1gJK"
   },
   "outputs": [],
   "source": [
    "def generate_data_csv(csv_file, sensor_num=10, method=0, use_optimal=True, sensors_dir='./optimal_sensors/'):\n",
    "    \"\"\"\n",
    "    Read CSV data and generate interpolated input and ground truth data.\n",
    "\n",
    "    Parameters:\n",
    "      csv_file: CSV file path (e.g., '0deg_1.csv')\n",
    "      sensor_num: Number of sensors per sample (default is 10)\n",
    "      method: Data method number, used to select corresponding optimal sensor positions\n",
    "      use_optimal: Whether to use optimal sensor positions, False uses uniform distribution\n",
    "      sensors_dir: Directory for optimal sensor position files\n",
    "\n",
    "    Returns:\n",
    "      X: Interpolation results and sensor mask, shape (N, 15, 15, 3)\n",
    "         Channel 0: Interpolated field of U component\n",
    "         Channel 1: Interpolated field of V component\n",
    "         Channel 2: Sensor position mask (1 at sensor points, 0 elsewhere)\n",
    "      y: Ground truth data, containing complete U, V fields, shape (N, 15, 15, 2)\n",
    "    \"\"\"\n",
    "    # Read CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Get all unique times corresponding to samples\n",
    "    times = np.sort(df['Time'].unique())\n",
    "    num_samples = len(times)\n",
    "\n",
    "    # Get grid x and y coordinates (assuming all samples have consistent X and Y coordinates)\n",
    "    x_coords = np.sort(df['X'].unique())\n",
    "    y_coords = np.sort(df['Y'].unique())\n",
    "\n",
    "    # Build 15√ó15 grid\n",
    "    xv, yv = np.meshgrid(x_coords, y_coords)\n",
    "    grid_shape = xv.shape  # Should be (15, 15)\n",
    "\n",
    "    # Pre-allocate output arrays\n",
    "    X = np.zeros((num_samples, grid_shape[0], grid_shape[1], 3), dtype=np.float32)\n",
    "    y = np.zeros((num_samples, grid_shape[0], grid_shape[1], 2), dtype=np.float32)\n",
    "\n",
    "    # Select sensor position strategy\n",
    "    if use_optimal:\n",
    "        try:\n",
    "            sensor_positions = load_optimal_sensors(method, sensor_num, sensors_dir)\n",
    "            print(f\"Using optimal sensor positions (Method {method})\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "            print(\"Falling back to uniform distribution sensor positions\")\n",
    "            sensor_positions = get_uniform_sensor_positions_center(grid_shape, sensor_num)\n",
    "    else:\n",
    "        sensor_positions = get_uniform_sensor_positions_center(grid_shape, sensor_num)\n",
    "        print(\"Using uniform distribution sensor positions\")\n",
    "\n",
    "    print(f\"Sensor positions: {sensor_positions[:5]}...\")  # Show first 5 positions\n",
    "\n",
    "    # Iterate through each sample (each unique Time value)\n",
    "    for i, t in enumerate(tqdm(times, desc=\"Processing samples\")):\n",
    "        # Select data for current time\n",
    "        df_t = df[df['Time'] == t]\n",
    "\n",
    "        # Use pivot to convert scattered data to 2D grid data\n",
    "        U_grid = df_t.pivot(index='Y', columns='X', values='U').values  # shape (15,15)\n",
    "        V_grid = df_t.pivot(index='Y', columns='X', values='V').values  # shape (15,15)\n",
    "\n",
    "        # Save ground truth data\n",
    "        y[i, :, :, 0] = U_grid\n",
    "        y[i, :, :, 1] = V_grid\n",
    "\n",
    "        # Extract sensor data: get U and V values at sensor positions separately\n",
    "        sensor_data_U = U_grid[sensor_positions[:, 0], sensor_positions[:, 1]]\n",
    "        sensor_data_V = V_grid[sensor_positions[:, 0], sensor_positions[:, 1]]\n",
    "\n",
    "        # Get real physical coordinates corresponding to sensors\n",
    "        sensor_coords = np.column_stack((\n",
    "            xv[sensor_positions[:, 0], sensor_positions[:, 1]],\n",
    "            yv[sensor_positions[:, 0], sensor_positions[:, 1]]\n",
    "        ))\n",
    "\n",
    "        # Interpolate U and V components separately using griddata\n",
    "        grid_U = griddata(sensor_coords, sensor_data_U, (xv, yv), method='nearest')\n",
    "        grid_V = griddata(sensor_coords, sensor_data_V, (xv, yv), method='nearest')\n",
    "\n",
    "        # Construct sensor mask: assign 1 at sensor_positions, 0 elsewhere\n",
    "        mask = np.zeros(grid_shape, dtype=np.float32)\n",
    "        mask[sensor_positions[:, 0], sensor_positions[:, 1]] = 1\n",
    "\n",
    "        # Save interpolation results and mask to X\n",
    "        X[i, :, :, 0] = grid_U\n",
    "        X[i, :, :, 1] = grid_V\n",
    "        X[i, :, :, 2] = mask\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1750958625473,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "ZfX9PmER8HAV"
   },
   "outputs": [],
   "source": [
    "def visualize_interpolation(X, y, csv_file, sample_index=0):\n",
    "    \"\"\"\n",
    "    Visualize U, V fields and sensor positions before and after interpolation for a sample.\n",
    "\n",
    "    Parameters:\n",
    "      X: Interpolated data and sensor mask (shape [num_samples, 15,15,3])\n",
    "      y: Ground truth data (shape [num_samples, 15,15,2])\n",
    "      csv_file: Original CSV file path, used to extract grid coordinates\n",
    "      sample_index: Index of sample to visualize, default is 0\n",
    "    \"\"\"\n",
    "    # Get grid coordinates from CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    x_coords = np.sort(df['X'].unique())\n",
    "    y_coords = np.sort(df['Y'].unique())\n",
    "    extent = (x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max())\n",
    "\n",
    "    # Extract interpolated data and ground truth for specified sample\n",
    "    interp_U = X[sample_index, :, :, 0]\n",
    "    interp_V = X[sample_index, :, :, 1]\n",
    "    mask     = X[sample_index, :, :, 2]\n",
    "    gt_U     = y[sample_index, :, :, 0]\n",
    "    gt_V     = y[sample_index, :, :, 1]\n",
    "\n",
    "    # Plot: left side shows ground truth, right side shows interpolation results; another plot shows sensor positions\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    # Ground Truth U\n",
    "    im0 = axs[0, 0].imshow(gt_U, origin='lower', extent=extent)\n",
    "    axs[0, 0].set_title(\"Ground Truth U\")\n",
    "    axs[0, 0].set_xlabel(\"X\")\n",
    "    axs[0, 0].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im0, ax=axs[0, 0])\n",
    "\n",
    "    # Interpolated U\n",
    "    im1 = axs[0, 1].imshow(interp_U, origin='lower', extent=extent)\n",
    "    axs[0, 1].set_title(\"Interpolated U\")\n",
    "    axs[0, 1].set_xlabel(\"X\")\n",
    "    axs[0, 1].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im1, ax=axs[0, 1])\n",
    "\n",
    "    # Sensor mask\n",
    "    im2 = axs[0, 2].imshow(mask, origin='lower', extent=extent)\n",
    "    axs[0, 2].set_title(\"Sensor Mask\")\n",
    "    axs[0, 2].set_xlabel(\"X\")\n",
    "    axs[0, 2].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im2, ax=axs[0, 2])\n",
    "\n",
    "    # Ground Truth V\n",
    "    im3 = axs[1, 0].imshow(gt_V, origin='lower', extent=extent)\n",
    "    axs[1, 0].set_title(\"Ground Truth V\")\n",
    "    axs[1, 0].set_xlabel(\"X\")\n",
    "    axs[1, 0].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im3, ax=axs[1, 0])\n",
    "\n",
    "    # Interpolated V\n",
    "    im4 = axs[1, 1].imshow(interp_V, origin='lower', extent=extent)\n",
    "    axs[1, 1].set_title(\"Interpolated V\")\n",
    "    axs[1, 1].set_xlabel(\"X\")\n",
    "    axs[1, 1].set_ylabel(\"Y\")\n",
    "    fig.colorbar(im4, ax=axs[1, 1])\n",
    "\n",
    "    # Overlay sensor positions on ground truth U plot\n",
    "    axs[1, 2].imshow(gt_U, origin='lower', extent=extent)\n",
    "    # Get grid indices where sensors are located based on mask\n",
    "    sensor_rows, sensor_cols = np.where(mask == 1)\n",
    "    # Map grid indices to physical coordinates (assuming x_coords and y_coords correspond to columns and rows respectively)\n",
    "    sensor_x = x_coords[sensor_cols]\n",
    "    sensor_y = y_coords[sensor_rows]\n",
    "    axs[1, 2].scatter(sensor_x, sensor_y, color='red', label=\"Sensors\")\n",
    "    axs[1, 2].set_title(\"Sensors on Ground Truth U\")\n",
    "    axs[1, 2].set_xlabel(\"X\")\n",
    "    axs[1, 2].set_ylabel(\"Y\")\n",
    "    axs[1, 2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWafvUgN8OfS"
   },
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M6Kq1yl2F9k"
   },
   "outputs": [],
   "source": [
    "method=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5LZXLXsE1W6",
    "outputId": "2503798a-dd6b-4175-a550-46899d9adde7"
   },
   "outputs": [],
   "source": [
    "# Generate data, sensor number is fixed at 10 here\n",
    "sensor_num = [5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for i in range(len(sensor_num)):\n",
    "  X1, y1 = generate_data_csv(f'{dir}45deg_1.csv', sensor_num=sensor_num[i], method=0, use_optimal=True, sensors_dir=sensors_dir)\n",
    "  X2, y2 = generate_data_csv(f'{dir}45deg_2.csv', sensor_num=sensor_num[i], method=0, use_optimal=True, sensors_dir=sensors_dir)\n",
    "  X3, y3 = generate_data_csv(f'{dir}45deg_3.csv', sensor_num=sensor_num[i], method=0, use_optimal=True, sensors_dir=sensors_dir)\n",
    "  # visualize_interpolation(X1, y1, f'{dir}0deg_1.csv', sample_index=1)\n",
    "  # Generate perturbed test set data\n",
    "  # grid_shape = X3.shape[1:3]  # Assume (15, 15)\n",
    "  # X3_perturbed = generate_perturbed_test_data(X3, y3, grid_shape, max_perturbation=1)\n",
    "  grid1_shape = X1.shape[1:3]  # Assume (15, 15)\n",
    "  grid2_shape = X2.shape[1:3]\n",
    "  grid3_shape = X3.shape[1:3]\n",
    "  X1_perturbed = generate_perturbed_test_data(X1, y1, grid1_shape, max_perturbation=1)\n",
    "  X2_perturbed = generate_perturbed_test_data(X2, y2, grid2_shape, max_perturbation=1)\n",
    "  X3_perturbed = generate_perturbed_test_data(X3, y3, grid3_shape, max_perturbation=1)\n",
    "\n",
    "  # Visualize comparison of original and perturbed (optional)\n",
    "  # visualize_perturbation(X3, X3_perturbed, y3, sample_index=1)\n",
    "\n",
    "  # np.save(f'{dir}45deg_x1_data_{sensor_num[i]}.npy', X1)\n",
    "  # np.save(f'{dir}45deg_y1_data_{sensor_num[i]}.npy', y1)\n",
    "  # np.save(f'{dir}45deg_x2_data_{sensor_num[i]}.npy', X2)\n",
    "  # np.save(f'{dir}45deg_y2_data_{sensor_num[i]}.npy', y2)\n",
    "  # np.save(f'{dir}45deg_x3_data_{sensor_num[i]}.npy', X3)\n",
    "  # np.save(f'{dir}45deg_y3_data_{sensor_num[i]}.npy', y3)\n",
    "  # print(f'num sensor = {sensor_num[i]}: Data saved')\n",
    "  # np.save(f'{dir}45deg_x1_perturbed_data_{sensor_num[i]}.npy', X1_perturbed)\n",
    "  # np.save(f'{dir}45deg_x2_perturbed_data_{sensor_num[i]}.npy', X2_perturbed)\n",
    "  # np.save(f'{dir}45deg_x3_perturbed_data_{sensor_num[i]}.npy', X3_perturbed)\n",
    "  print(f'num sensor = {sensor_num[i]}: Perturbed data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKGcVjNi15EO"
   },
   "source": [
    "method=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14279,
     "status": "error",
     "timestamp": 1750327881918,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "SYKX4VSEkSV6",
    "outputId": "609e78d1-631c-4dc9-e055-8224c4472160"
   },
   "outputs": [],
   "source": [
    "# Generate data, sensor number is fixed at 10 here\n",
    "sensor_num = [5, 10, 15, 20, 25, 30]\n",
    "sensor_num = [5]\n",
    "\n",
    "for i in range(len(sensor_num)):\n",
    "  X1, y1 = generate_data_csv(f'{dir}22deg_1.csv', sensor_num=sensor_num[i])\n",
    "  X2, y2 = generate_data_csv(f'{dir}22deg_2.csv', sensor_num=sensor_num[i])\n",
    "  # X3, y3 = generate_data_csv(f'{dir}45deg_3.csv', sensor_num=sensor_num[i])\n",
    "  visualize_interpolation(X1, y1, f'{dir}22deg_1.csv', sample_index=1)\n",
    "  # # Generate perturbed test set data\n",
    "  # grid1_shape = X1.shape[1:3]  # Assume (15, 15)\n",
    "  # grid2_shape = X2.shape[1:3]\n",
    "  # X1_perturbed = generate_perturbed_test_data(X1, y1, grid1_shape, max_perturbation=1)\n",
    "  # X2_perturbed = generate_perturbed_test_data(X2, y2, grid2_shape, max_perturbation=1)\n",
    "\n",
    "  # Visualize comparison of original and perturbed (optional)\n",
    "  # visualize_perturbation(X3, X3_perturbed, y3, sample_index=1)\n",
    "\n",
    "  # np.save(f'{dir}22deg_x1_data_{sensor_num[i]}.npy', X1)\n",
    "  # np.save(f'{dir}22deg_y1_data_{sensor_num[i]}.npy', y1)\n",
    "  # np.save(f'{dir}22deg_x2_data_{sensor_num[i]}.npy', X2)\n",
    "  # np.save(f'{dir}22deg_y2_data_{sensor_num[i]}.npy', y2)\n",
    "  # np.save(f'{dir}45deg_x3_data_{sensor_num[i]}.npy', X3)\n",
    "  # np.save(f'{dir}45deg_y3_data_{sensor_num[i]}.npy', y3)\n",
    "  # print(f'num sensor = {sensor_num[i]}: Data saved')\n",
    "  # np.save(f'{dir}22deg_x1_perturbed_data_{sensor_num[i]}.npy', X1_perturbed)\n",
    "  # np.save(f'{dir}22deg_x2_perturbed_data_{sensor_num[i]}.npy', X2_perturbed)\n",
    "  # print(f'num sensor = {sensor_num[i]}: Perturbed data saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzYR_bZnrOK6"
   },
   "outputs": [],
   "source": [
    "def visualize_interpolation(X, y, csv_file, num_samples=1):\n",
    "    \"\"\"\n",
    "    Visualize ground truth U, V fields for multiple samples.\n",
    "\n",
    "    Parameters:\n",
    "      X: Interpolated data and sensor mask (shape [num_samples, 15,15,3])\n",
    "      y: Ground truth data (shape [num_samples, 15,15,2])\n",
    "      csv_file: Original CSV file path, used to extract grid coordinates\n",
    "      num_samples: Number of samples to display, default is 1\n",
    "    \"\"\"\n",
    "    # Get grid coordinates from CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    x_coords = np.sort(df['X'].unique())\n",
    "    y_coords = np.sort(df['Y'].unique())\n",
    "    extent = (x_coords.min(), x_coords.max(), y_coords.min(), y_coords.max())\n",
    "\n",
    "    # Create subplot layout: each sample occupies 2 rows (U and V), number of columns equals number of samples\n",
    "    fig, axs = plt.subplots(2, num_samples, figsize=(6*num_samples, 10))\n",
    "\n",
    "    # If only one sample, axs needs to be reshaped\n",
    "    if num_samples == 1:\n",
    "        axs = axs.reshape(2, 1)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Extract ground truth for the i-th sample\n",
    "        gt_U = y[i, :, :, 0]\n",
    "        gt_V = y[i, :, :, 1]\n",
    "\n",
    "        # Ground Truth U\n",
    "        im0 = axs[0, i].imshow(gt_U, origin='lower', extent=extent)\n",
    "        axs[0, i].set_title(f\"Sample {i+1} - Ground Truth U\")\n",
    "        axs[0, i].set_xlabel(\"X\")\n",
    "        axs[0, i].set_ylabel(\"Y\")\n",
    "        fig.colorbar(im0, ax=axs[0, i])\n",
    "\n",
    "        # Ground Truth V\n",
    "        im1 = axs[1, i].imshow(gt_V, origin='lower', extent=extent)\n",
    "        axs[1, i].set_title(f\"Sample {i+1} - Ground Truth V\")\n",
    "        axs[1, i].set_xlabel(\"X\")\n",
    "        axs[1, i].set_ylabel(\"Y\")\n",
    "        fig.colorbar(im1, ax=axs[1, i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "executionInfo": {
     "elapsed": 61860,
     "status": "ok",
     "timestamp": 1750330212585,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "qWLivFWisGLC",
    "outputId": "cee5021a-32ba-4686-eb9f-f736ab1d6861"
   },
   "outputs": [],
   "source": [
    "# Generate data, sensor number is fixed at 10 here\n",
    "sensor_num = [5]\n",
    "\n",
    "for i in range(len(sensor_num)):\n",
    "  X1, y1 = generate_data_csv(f'{dir}22deg_1.csv', sensor_num=sensor_num[i])\n",
    "  # X2, y2 = generate_data_csv(f'{dir}22deg_2.csv', sensor_num=sensor_num[i])\n",
    "  # X3, y3 = generate_data_csv(f'{dir}45deg_3.csv', sensor_num=sensor_num[i])\n",
    "  visualize_interpolation(X1, y1, f'{dir}22deg_1.csv', num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 66532,
     "status": "ok",
     "timestamp": 1750330437956,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "mc5Aejx4s9fM",
    "outputId": "ba405232-bc0e-4e8d-c82f-a0e799a95b7c"
   },
   "outputs": [],
   "source": [
    "# Generate data, sensor number is fixed at 10 here\n",
    "sensor_num = [5]\n",
    "\n",
    "for i in range(len(sensor_num)):\n",
    "  X1, y1 = generate_data_csv(f'{dir}0deg_1.csv', sensor_num=sensor_num[i])\n",
    "  # X2, y2 = generate_data_csv(f'{dir}22deg_2.csv', sensor_num=sensor_num[i])\n",
    "  # X3, y3 = generate_data_csv(f'{dir}45deg_3.csv', sensor_num=sensor_num[i])\n",
    "  visualize_interpolation(X1, y1, f'{dir}0deg_1.csv', num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMjASSNs6GrX"
   },
   "source": [
    "# Batch Generate Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1750958637987,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "UhslErpA6Ixc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def batch_generate_all_data(base_dir='./dataset/',\n",
    "                           sensors_dir='./optimal_sensors/',\n",
    "                           output_dir='./generated_data/'):\n",
    "    \"\"\"\n",
    "    Batch generate data for all method, sensor_num, CSV combinations\n",
    "\n",
    "    Parameters:\n",
    "        base_dir: Directory where CSV files are located\n",
    "        sensors_dir: Directory for optimal sensor position files\n",
    "        output_dir: Directory for output npy files\n",
    "    \"\"\"\n",
    "\n",
    "    # Define all parameters\n",
    "    csv_files = [\n",
    "        '45deg_1.csv', '45deg_2.csv', '45deg_3.csv',\n",
    "        '0deg_1.csv', '0deg_2.csv', '0deg_3.csv',\n",
    "        '22deg_1.csv', '22deg_2.csv'\n",
    "    ]\n",
    "\n",
    "    sensor_nums = [5, 10, 15, 20, 25, 30]\n",
    "    methods = [0, 1]\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Count total number of tasks\n",
    "    total_tasks = len(methods) * len(sensor_nums) * len(csv_files)\n",
    "    print(f\"Total tasks to process: {total_tasks}\")\n",
    "\n",
    "    # Start batch generation\n",
    "    task_count = 0\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting to process Method {method}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        for sensor_num in sensor_nums:\n",
    "            print(f\"\\n--- Method {method}, Sensor_num {sensor_num} ---\")\n",
    "\n",
    "            # Check if corresponding optimal sensor position file exists\n",
    "            try:\n",
    "                positions = load_optimal_sensors(method, sensor_num, sensors_dir)\n",
    "                print(f\"Successfully loaded optimal sensor positions: Method {method}, Sensor_num {sensor_num}\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Warning: {e}\")\n",
    "                print(f\"Skipping Method {method}, Sensor_num {sensor_num}\")\n",
    "                continue\n",
    "\n",
    "            for csv_file in csv_files:\n",
    "                task_count += 1\n",
    "                csv_path = os.path.join(base_dir, csv_file)\n",
    "\n",
    "                # Extract identifier from CSV filename (remove .csv suffix)\n",
    "                csv_name = csv_file.replace('.csv', '')\n",
    "\n",
    "                print(f\"[{task_count}/{total_tasks}] Processing: {csv_file}\")\n",
    "\n",
    "                try:\n",
    "                    # Generate data\n",
    "                    X, y = generate_data_csv(\n",
    "                        csv_path,\n",
    "                        sensor_num=sensor_num,\n",
    "                        method=method,\n",
    "                        use_optimal=True,\n",
    "                        sensors_dir=sensors_dir\n",
    "                    )\n",
    "\n",
    "                    # Generate perturbed data\n",
    "                    grid_shape = X.shape[1:3]  # (15, 15)\n",
    "                    X_perturbed = generate_perturbed_test_data(X, y, grid_shape, max_perturbation=1)\n",
    "\n",
    "                    # Build filename\n",
    "                    # Format: {csv_name}_method{method}_sensor{sensor_num}_{type}.npy\n",
    "                    base_filename = f\"{csv_name}_method{method}_sensor{sensor_num}\"\n",
    "\n",
    "                    x_filename = f\"{base_filename}_X.npy\"\n",
    "                    y_filename = f\"{base_filename}_y.npy\"\n",
    "                    x_perturbed_filename = f\"{base_filename}_X_perturbed.npy\"\n",
    "\n",
    "                    # Save files\n",
    "                    np.save(os.path.join(output_dir, x_filename), X)\n",
    "                    np.save(os.path.join(output_dir, y_filename), y)\n",
    "                    np.save(os.path.join(output_dir, x_perturbed_filename), X_perturbed)\n",
    "\n",
    "                    print(f\"  ‚úÖ Successfully saved:\")\n",
    "                    print(f\"     - {x_filename}\")\n",
    "                    print(f\"     - {y_filename}\")\n",
    "                    print(f\"     - {x_perturbed_filename}\")\n",
    "                    print(f\"     Data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Processing failed: {csv_file} - {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Batch generation completed! Total tasks processed: {task_count}\")\n",
    "    print(f\"Files saved in: {output_dir}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def list_generated_files(output_dir='./generated_data/', show_details=True):\n",
    "    \"\"\"\n",
    "    List all generated files, organized by category\n",
    "\n",
    "    Parameters:\n",
    "        output_dir: File directory\n",
    "        show_details: Whether to show detailed file information\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Directory does not exist: {output_dir}\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(output_dir) if f.endswith('.npy')]\n",
    "    files.sort()\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No .npy files found in directory: {output_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nGenerated file list (total {len(files)} files):\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Group by method and sensor_num\n",
    "    from collections import defaultdict\n",
    "    grouped_files = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for file in files:\n",
    "        # Parse filename\n",
    "        parts = file.replace('.npy', '').split('_')\n",
    "\n",
    "        if 'method' in file and 'sensor' in file:\n",
    "            # Find positions of method and sensor\n",
    "            method_idx = next(i for i, part in enumerate(parts) if part.startswith('method'))\n",
    "            sensor_idx = next(i for i, part in enumerate(parts) if part.startswith('sensor'))\n",
    "\n",
    "            method = parts[method_idx].replace('method', '')\n",
    "            sensor = parts[sensor_idx].replace('sensor', '')\n",
    "\n",
    "            # CSV name (parts before method)\n",
    "            csv_parts = parts[:method_idx]\n",
    "            csv_name = '_'.join(csv_parts)\n",
    "\n",
    "            # File type (parts after sensor)\n",
    "            type_parts = parts[sensor_idx+1:]\n",
    "            file_type = '_'.join(type_parts) if type_parts else 'X'\n",
    "\n",
    "            grouped_files[f\"Method{method}\"][f\"Sensor{sensor}\"].append({\n",
    "                'csv': csv_name,\n",
    "                'type': file_type,\n",
    "                'filename': file\n",
    "            })\n",
    "\n",
    "    # Display grouped results\n",
    "    for method_key in sorted(grouped_files.keys()):\n",
    "        print(f\"\\nüìÅ {method_key}:\")\n",
    "\n",
    "        for sensor_key in sorted(grouped_files[method_key].keys()):\n",
    "            print(f\"  üìÅ {sensor_key}:\")\n",
    "\n",
    "            # Group by CSV\n",
    "            csv_groups = defaultdict(list)\n",
    "            for file_info in grouped_files[method_key][sensor_key]:\n",
    "                csv_groups[file_info['csv']].append(file_info)\n",
    "\n",
    "            for csv_name in sorted(csv_groups.keys()):\n",
    "                files_for_csv = csv_groups[csv_name]\n",
    "                types = [f['type'] for f in files_for_csv]\n",
    "\n",
    "                print(f\"    üìÑ {csv_name}: {', '.join(sorted(types))}\")\n",
    "\n",
    "                if show_details:\n",
    "                    for file_info in files_for_csv:\n",
    "                        filepath = os.path.join(output_dir, file_info['filename'])\n",
    "                        if os.path.exists(filepath):\n",
    "                            size_mb = os.path.getsize(filepath) / (1024*1024)\n",
    "                            print(f\"       - {file_info['filename']} ({size_mb:.2f} MB)\")\n",
    "\n",
    "def generate_file_mapping(output_dir='./generated_data/'):\n",
    "    \"\"\"\n",
    "    Generate file mapping table for convenient use later\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(output_dir) if f.endswith('.npy')]\n",
    "\n",
    "    mapping = {}\n",
    "\n",
    "    for file in files:\n",
    "        # Parse filename\n",
    "        parts = file.replace('.npy', '').split('_')\n",
    "\n",
    "        if 'method' in file and 'sensor' in file:\n",
    "            method_idx = next(i for i, part in enumerate(parts) if part.startswith('method'))\n",
    "            sensor_idx = next(i for i, part in enumerate(parts) if part.startswith('sensor'))\n",
    "\n",
    "            method = int(parts[method_idx].replace('method', ''))\n",
    "            sensor = int(parts[sensor_idx].replace('sensor', ''))\n",
    "\n",
    "            csv_parts = parts[:method_idx]\n",
    "            csv_name = '_'.join(csv_parts)\n",
    "\n",
    "            type_parts = parts[sensor_idx+1:]\n",
    "            file_type = '_'.join(type_parts) if type_parts else 'X'\n",
    "\n",
    "            key = (method, sensor, csv_name, file_type)\n",
    "            mapping[key] = file\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def load_data_by_key(method, sensor_num, csv_name, data_type='X', output_dir='./generated_data/'):\n",
    "    \"\"\"\n",
    "    Load corresponding data based on key values\n",
    "\n",
    "    Parameters:\n",
    "        method: Method number (0 or 1)\n",
    "        sensor_num: Number of sensors\n",
    "        csv_name: CSV name (without .csv suffix)\n",
    "        data_type: Data type ('X', 'y', 'X_perturbed')\n",
    "        output_dir: Data directory\n",
    "\n",
    "    Returns:\n",
    "        data: Loaded numpy array\n",
    "    \"\"\"\n",
    "    filename = f\"{csv_name}_method{method}_sensor{sensor_num}_{data_type}.npy\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File does not exist: {filepath}\")\n",
    "\n",
    "    return np.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8224326,
     "status": "ok",
     "timestamp": 1750966880956,
     "user": {
      "displayName": "Chow Young",
      "userId": "08677253659012730157"
     },
     "user_tz": -480
    },
    "id": "B_zGlALk6Xt7",
    "outputId": "d024a891-f7d2-4a34-9c61-fd91bf6aca39"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function: demonstration of how to use\n",
    "\"\"\"\n",
    "print(\"Starting batch data generation...\")\n",
    "\n",
    "# 1. Batch generate all data\n",
    "batch_generate_all_data(\n",
    "    base_dir='/content/drive/MyDrive/TorchDA/dataset',  # Adjust to your CSV file directory\n",
    "    sensors_dir='/content/drive/MyDrive/TorchDA/position',\n",
    "    output_dir='/content/drive/MyDrive/TorchDA/optimal_dataset'\n",
    ")\n",
    "\n",
    "# 2. List generated files\n",
    "print(\"\\nViewing generated files:\")\n",
    "list_generated_files('./generated_data/')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
